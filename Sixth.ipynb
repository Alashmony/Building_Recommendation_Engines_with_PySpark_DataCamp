{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e00f1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width : 99% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.output_result {width : 99% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.jp-Notebook {--jp-notebook-max-width: 99%;}/style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container {width : 99% !important;}</style>\"))\n",
    "display(HTML(\"<style>.output_result {width : 99% !important;}</style>\"))\n",
    "display(HTML(\"<style>.jp-Notebook {--jp-notebook-max-width: 99%;}/style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13e68e1e-003c-4fd9-836e-262652f3493d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://AAsmny-lt-11105.itworx.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x21086f98040>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext().getOrCreate()\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "sc\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed90cdf8-b45b-40aa-9e69-f48187c3d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f727b75-72b0-4c24-9d01-dbbae4653a6f",
   "metadata": {},
   "source": [
    "# Recommendations Are Everywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3d3e49-6a24-421a-a946-2cfdb4e1d7c0",
   "metadata": {},
   "source": [
    "## Why learn how to build recommendation engines?\n",
    "1. Why learn how to build recommendation engines?\n",
    "\n",
    "Hi. Welcome to this course on building recommendation engines using Alternating Least Squares or \"ALS\" in PySpark.\n",
    "\n",
    "2. What recommendations look like\n",
    "00:09 - 00:34\n",
    "You're probably already familiar with the output of these types of recommendation engines where a website tells you something along the lines of, \"If you like that, then you'll probably like this.\" You've likely seen these types of recommendations on your favorite retail or media streaming websites. These recommendations are generated through different types of data that you as a user or customer provide either directly or indirectly.\n",
    "\n",
    "3. Learning about you\n",
    "\n",
    "When you purchase something online, or watch a movie, or even read an article, you are often given a chance to rate that item on a scale of 1 to 5 stars, a thumbs up or thumbs down, or some other type of rating scale. Based on your feedback from these types of rating systems, companies can learn a lot about your preferences, and offer you recommendations based on preferences of users that are similar to you.\n",
    "\n",
    "4. How recommendation engines work\n",
    "\n",
    "For example, if your movie streaming service sees that you liked Dark Knight and Iron Man, and did not like Tangled, and it also sees\n",
    "\n",
    "5. How recommendation engines work\n",
    "\n",
    "other users that also liked Dark Knight and Iron Man and also did not like Tangled, the ALS algorithm would see that you and these other users have\n",
    "\n",
    "6. How recommendation engines work\n",
    "\n",
    "similar tastes. It would then look at the movies that you have not yet seen, and see which ones are the\n",
    "\n",
    "7. How recommendation engines work\n",
    "\n",
    "highest rated among those similar users, and offer them as\n",
    "\n",
    "8. How recommendation engines work\n",
    "\n",
    "recommendations to you. This is why websites will often say things like, \"Because you liked that movie, we think you'll like this movie.\" Or \"Users like you also watched this movie.\"\n",
    "\n",
    "9. The Power of Recommendation Engines\n",
    "\n",
    "These types of rating systems are extremely powerful. In fact, an article published by McKinsey & Company in October of 2013 stated that 35% of what customers buy on Amazon and 75% of what they watch on Netflix come from product recommendations based on algorithms such as the one you are going to be learning in this course. That's a powerful use of data, and with this course, you will learn how to do this. In addition to this, there are alternate uses for recommendation algorithms that can be extremely useful for purposes as broad as feature space reduction, image compression, mathematical user and product grouping, latent feature discovery and you're going to learn some of these in this course.\n",
    "\n",
    "10. Prerequisites\n",
    "\n",
    "This tutorial is intended for those that have experience with Spark and Python, and understand the fundamentals of machine learning. If needed, some good introductory resources are DataCamp's Introduction to PySpark course, their Intermediate Python for Data Science course, and their Supervised Machine Learning with Python's SciKitLearn course.\n",
    "\n",
    "11. Let's practice!\n",
    "\n",
    "Let's jump in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7440441-be44-415a-a8c6-22a33249e679",
   "metadata": {},
   "source": [
    "### See the power of a recommendation engine\n",
    "Taylor and Jane both like watching movies. Taylor only likes dramas, comedies, and romances. Jane likes only action, adventure, and otherwise exciting films. One of the greatest benefits of ALS-based recommendation engines is that they can identify movies or items that users will like, even if they themselves think that they might not like them. Take a look at the movie ratings that Taylor and Jane have provided below. It would stand to reason that their different preferences would generate different recommendations.\n",
    "\n",
    "This course touches on a lot of concepts you may have forgotten, so if you ever need a quick refresher, download the [PySpark Cheat Sheet](https://datacamp-community-prod.s3.amazonaws.com/65076e3c-9df1-40d5-a0c2-36294d9a3ca9) and keep it handy!\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Take a look at `TJ_ratings` using the `.show()` method and any other methods you prefer to see how each of them rated the various movies they've seen.\n",
    "- Input user names into the `get_ALS_recs()` function provided to see what movies ALS recommends for Jane and Taylor based on the ratings provided. Do the ratings make sense to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a33ddc",
   "metadata": {},
   "source": [
    "#### To get code for the function, used:\n",
    "`import inspect`\n",
    "\n",
    "`lines = inspect.getsource(get_ALS_recs)`\n",
    "\n",
    "`print(lines)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85dffada",
   "metadata": {},
   "outputs": [],
   "source": [
    "TJ_pandas = pd.DataFrame(columns = ['user_name' ,'movie_name' , 'rating'] , data = np.array([ ['Taylor'       ,          'Twilight'    , 4.9]\n",
    ",     ['Taylor'   ,    'A Walk to Remember' ,    4.5]\n",
    ",     ['Taylor'   ,          'The Notebook'   ,  5.0]\n",
    ",     ['Taylor'  ,'Raiders of the Lost Ark'   ,  1.2]\n",
    ",     ['Taylor'  ,         'The Terminator'  ,   1.0]\n",
    ",     ['Taylor'   ,        'Mrs. Doubtfire' ,    1.0]\n",
    ",     [  'Jane'  ,               'Iron Man'  ,   4.8]\n",
    ",      [ 'Jane' , 'Raiders of the Lost Ark'  ,   4.9]\n",
    ",      [ 'Jane' ,          'The Terminator'  ,   4.6]\n",
    ",    [   'Jane'  ,              'Anchorman'   ,  1.2]\n",
    ",   [   'Jane'   ,          'Pretty Woman'   ,  1.0]\n",
    ",   [   'Jane'   ,             'Toy Story'  ,   1.2]]))\n",
    "\n",
    "TJ_ratings = spark.createDataFrame(TJ_pandas)\n",
    "TJ_ALS_recs = spark.createDataFrame(pd.DataFrame(columns = ['userId','pred_rating' , 'title', 'genres'], data = np.array([  [ 'Taylor'   ,      3.89  , 'Seven Pounds (2008)',           'Drama']\n",
    ",   ['Taylor'       ,  3.61 ,     'Cure, The (1995) ',          'Drama']\n",
    ",   ['Taylor'     ,    3.55 , 'Kiss Me, Guido (1997 ',         'Comedy']\n",
    ",   ['Taylor'    ,     3.29 , 'You\\'ve Got Mail (199 ', 'Comedy|Romance']\n",
    ",   ['Taylor'    ,     3.27 , '10 Things I Hate Abo' , 'Comedy|Romance']\n",
    ",   ['Taylor'    ,     3.26 , 'Corrina, Corrina (19' , 'Comedy|Drama|R']\n",
    ",   [  'Jane'    ,     4.96 ,          'Fear (1996)' ,       'Thriller']\n",
    ",   [  'Jane'    ,     4.85 , 'Lord of the Rings: T'  ,'Adventure|Fant']\n",
    ",   [  'Jane'    ,     4.70 , 'Lord of the Rings: T ', 'Adventure|Fant']\n",
    ",   [  'Jane'    ,     4.55,  'No Holds Barred (198 ',         'Action']\n",
    ",  [  'Jane'    ,     4.54 , 'Lord of the Rings: T' , 'Action|Adventu']\n",
    ", [   'Jane'    ,     4.30 , 'Band of Brothers (20' , 'Action|Drama|W']\n",
    ",  [  'Jane'    ,     4.26  , 'Transformers (2007)' , 'Action|Sci-Fi|']\n",
    "])\n",
    "))\n",
    "\n",
    "\n",
    "def get_ALS_recs(list_of_user_names, recs = TJ_ALS_recs):\n",
    "    \"\"\" \n",
    "    Returns recommendations generated by ALS algorithm\n",
    "    \n",
    "    Parameters:\n",
    "      - list_of_user_names: List or array of user names\n",
    "      - recs: ALS-generated recommendations\n",
    "      \n",
    "    Returns: Pyspark dataframe of recommendations for user names submitted in list_of_usre_names.\n",
    "    \"\"\"\n",
    "    if len(list_of_user_names) == 0:\n",
    "        print (\"None\")\n",
    "    elif len(list_of_user_names) == 1:\n",
    "        print (recs[recs.userId == list_of_user_names[0]].sort_values(by=['user_name','pred_rating'], ascending = [0,0]))\n",
    "    elif len(list_of_user_names) == 2:\n",
    "        print (recs)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a876fd67-aa10-47e9-b17a-c317d3533b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------+\n",
      "|user_name|          movie_name|rating|\n",
      "+---------+--------------------+------+\n",
      "|   Taylor|            Twilight|   4.9|\n",
      "|   Taylor|  A Walk to Remember|   4.5|\n",
      "|   Taylor|        The Notebook|   5.0|\n",
      "|   Taylor|Raiders of the Lo...|   1.2|\n",
      "|   Taylor|      The Terminator|   1.0|\n",
      "|   Taylor|      Mrs. Doubtfire|   1.0|\n",
      "|     Jane|            Iron Man|   4.8|\n",
      "|     Jane|Raiders of the Lo...|   4.9|\n",
      "|     Jane|      The Terminator|   4.6|\n",
      "|     Jane|           Anchorman|   1.2|\n",
      "|     Jane|        Pretty Woman|   1.0|\n",
      "|     Jane|           Toy Story|   1.2|\n",
      "+---------+--------------------+------+\n",
      "\n",
      "DataFrame[userId: string, pred_rating: string, title: string, genres: string]\n"
     ]
    }
   ],
   "source": [
    "# View TJ_ratings\n",
    "TJ_ratings.show()\n",
    "\n",
    "# Generate recommendations for users\n",
    "get_ALS_recs([\"Taylor\",\"Jane\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c8e7bc-d6ab-439f-a787-9b3550779731",
   "metadata": {},
   "source": [
    "### Power of recommendation engines\n",
    "What is a reason for learning to build recommendation engines?\n",
    "\n",
    "Answer the question\n",
    "\n",
    "- Sales always go up with recommendations.\n",
    "\n",
    "- **Show users items/products relevant to them that they may not know are available.**\n",
    "\n",
    "- Customers always take recommendations.\n",
    "\n",
    "- Because other successful companies do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f4decb-0877-4815-ae40-8ba68b39d208",
   "metadata": {},
   "source": [
    "## Recommendation engine types and data types\n",
    "\n",
    "1. Recommendation engine types and data types\n",
    "\n",
    "In the world of recommendation engines, there are two basic types:\n",
    "\n",
    "2. Two types of recommendation engines:\n",
    "\n",
    "Collaborative-filtering engines and content-based filtering engines. Both aim to offer meaningful recommendations, but they do so in slightly different ways. Content-based filtering, as the name suggests, tries to understand the content, or {{1}}features of the items, and makes recommendations based on your preferences for those specific features. For example, a movie streaming service might go to great lengths to add descriptive tags to their movies such as the genre, whether it's animated or not, the language spoken in the movie, the decade it was filmed, and which actors were in it, etc. So when a user like you\n",
    "\n",
    "3. Two types of recommendation engines\n",
    "\n",
    "gives 5 stars to a really dramatic, Portuguese movie with specific actors from a specific decade, they can infer that you like movies like this and will also like\n",
    "\n",
    "4. Two types of recommendation engines\n",
    "\n",
    "other dramatic movies in Portuguese with those same actors,\n",
    "\n",
    "5. Two types of recommendation engines\n",
    "\n",
    "and recommend those movies to you. Collaborative filtering is a little bit different.\n",
    "\n",
    "6. Two types of recommendation engines\n",
    "\n",
    "As explained in the previous video, collaborative filtering is based on user similarity. However, unlike content-based filtering, manually-created tags are not necessary. The features and groupings are created mathematically from patterns in the ratings provided by users. When you provide ratings for a product or item, whether it be a thumbs up or thumbs down, or even if you just watch a video without even giving it a rating, you are providing meaningful insight about your preferences. From this behavior, the ALS algorithm can mathematically group you with similar users, predict your behavior, and help you have a more effective customer experience. While ALS can have content-based applications, this course will focus on it's application to collaborative filtering, but many of the principles of collaborative filtering can be applied to content-based applications.\n",
    "\n",
    "7. Two types of ratings\n",
    "\n",
    "Now let's talk about ratings. In the realm of recommendation engines, there are two main types of ratings:\n",
    "\n",
    "8. Two types of ratings\n",
    "\n",
    "Explicit ratings\n",
    "\n",
    "9. Two types of ratings\n",
    "\n",
    "and implicit ratings Explicit ratings are pretty straightforward. Examples of these are when you input\n",
    "\n",
    "10. Two types of ratings\n",
    "\n",
    "a number of stars or something like a thumbs up or thumbs down. These are explicit ratings because users explicitly state how much they like or dislike something. Implicit ratings are a little bit different. They are based on the passive tracking of your behavior, like the number of movies you've seen in different genres. Fundamentally, implicit ratings are generated from the frequency of your actions. For example, if you watch 30 movies, and of those 30 movies,\n",
    "\n",
    "11. Two types of ratings\n",
    "\n",
    "22 are action movies, and only 1 is a comedy, the low number of comedy views will be converted into\n",
    "\n",
    "12. Two types of ratings\n",
    "\n",
    "low confidence that you like comedies, and the high number of action movie views will be converted into\n",
    "\n",
    "13. Two types of ratings\n",
    "\n",
    "a high confidence that you like action movies. These probabilities are then used as ratings. The logic behind this is, in essence, the more you carry out a behavior, the higher the the likelihood that you like it, and thus a higher rating. Additionally, in some cases you may not have access to behavior counts like this. A simpler form of ratings that still works with the ALS algorithm is the use of simple binary ratings. Rather than having a count of user actions, binary ratings just show whether a user has done something, like watched a comedy, represented by a 1 or not watched a comedy, represented by a 0. These types of ratings aren't nearly as rich, but they still can provide meaningful insight and still work perfectly fine with the ALS algorithm.\n",
    "\n",
    "14. Let's practice!\n",
    "\n",
    "Now, let's look at some actual data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ec7ce4-5ced-429c-b59a-cdafa10125d3",
   "metadata": {},
   "source": [
    "### Collaborative vs content-based filtering\n",
    "Below are statements that are often used when providing recommendations. Select the one that DOES NOT indicate collaborative filtering.\n",
    "\n",
    "Answer the question\n",
    "\n",
    "- **\"Because you liked that product, we think you'll like this product.\"**\n",
    "\n",
    "- \"Users that bought that also bought this.\"\n",
    "\n",
    "- \"Other people like you also liked this movie.\"\n",
    "\n",
    "- \"80% of your friends liked this movie, we think you'll like it too.\"\n",
    "\n",
    "- \"Here are top choices from similar users.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeca54cc-382b-48f1-9c82-a143d71da91b",
   "metadata": {},
   "source": [
    "### Collaborative vs content based filtering part II\n",
    "Look at the df dataframe using the `.show()` method and/or the `.columns` method, and determine whether it is best suited for \"collaborative filtering\", \"content-based filtering\", or \"both\".\n",
    "\n",
    "- collaborative filtering\n",
    "- content-based filtering\n",
    "- **both**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4444242d-a778-45df-b92e-ca7179309363",
   "metadata": {},
   "source": [
    "### Implicit vs explicit data\n",
    "Recall the differences between implicit and explicit ratings. Take a look at the `df1` dataframe to understand whether the data includes implicit or explicit ratings data.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Use the `.columns` and `.show()` methods to get an idea of the data provided, and see if the data includes implicit or explicit ratings.\n",
    "- Type \"implicit\" or \"explicit\" based on whether you think this data contains \"implicit\" ratings or \"explicit\" ratings. Name your response answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5e1e914",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.csv(\"markus_ratings.csv\", inferSchema = True, header= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "695ccc71-c6f0-4536-86b3-461666dedddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+---------+\n",
      "|         Movie_Title|             Genre|Num_Views|\n",
      "+--------------------+------------------+---------+\n",
      "|        Finding Nemo|Animated Childrens|       12|\n",
      "|           Toy Story|Animated Childrens|        6|\n",
      "|            Iron Man|            Action|        1|\n",
      "|     Captain America|            Action|        1|\n",
      "|     The Incredibles|Animated Childrens|        9|\n",
      "|              Frozen|Animated Childrens|       22|\n",
      "|The Shawshank Red...|             Drama|        2|\n",
      "|  Rabbit Proof Fence|             Drama|        2|\n",
      "|Searching for Sug...|       Documentary|        3|\n",
      "|              Powder|             Drama|        1|\n",
      "|        The Fugitive|            Action|        2|\n",
      "+--------------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb61f0c0-f5f7-41fd-839b-39a0be4cfb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type \"implicit\" or \"explicit\"\n",
    "answer = \"implicit\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e803e64c-7fd9-45aa-ab13-ddfb3d0674ce",
   "metadata": {},
   "source": [
    "### Ratings data types\n",
    "Markus watches a lot of movies, including documentaries, superhero movies, classics, and dramas. Drawing on your previous experience with Spark, use the `markus_ratings` dataframe, which contains data on the number of times Markus has seen movies in various genres, and think about whether these are implicit or explicit ratings. Use the `groupBy()` method to determine which genre has the highest rating, which could likely influence what recommendations ALS would generate for Markus.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Use the `groupBy()` method to group the `markus_ratings` dataframe by `\"Genre\"`.\n",
    "- Apply the `.sum()` method to get the total number of movies watched for each genre.\n",
    "- Be sure to add the `.show()` method at the end to view the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68b7a70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "markus_ratings = df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c47c216d-d543-41e2-a1cd-d992e4dbf0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n",
      "|             Genre|sum(Num_Views)|\n",
      "+------------------+--------------+\n",
      "|             Drama|             5|\n",
      "|       Documentary|             3|\n",
      "|            Action|             4|\n",
      "|Animated Childrens|            49|\n",
      "+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group the data by \"Genre\"\n",
    "markus_ratings.groupBy(\"Genre\").sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f902f0b-2000-4648-915a-9099ae8a5bd9",
   "metadata": {},
   "source": [
    "## Uses for recommendation engines\n",
    "1. Uses for recommendation engines\n",
    "\n",
    "So far, we've only considered recommendations as the use case for the ALS algorithm, but there are other applications that are also useful. These include latent feature discovery, item grouping, dimensionality reduction and image compression. In this course we'll only talk about some of these. First let's talk about latent features. As mentioned earlier, people will go to great lengths to effectively categorize items. But some products span various categories making them difficult to organize. Movies are often like this. Horror movies can be comedies. Dramas can be satires. Documentaries can be romances or even mysteries. Because of this, they can sometimes be difficult to market. If we had a better understanding of how consumers categorize movies based on their experience watching them, we could add more power to marketing strategies. ALS can help with this.\n",
    "\n",
    "2. Basic factorization of ALS\n",
    "\n",
    "When we have a matrix that contains users and movie ratings, ALS\n",
    "\n",
    "3. Basic factorization of ALS (cont.)\n",
    "\n",
    "will factor that matrix into two matrices, one containing\n",
    "\n",
    "4. User matrix\n",
    "\n",
    "user information and the other containing\n",
    "\n",
    "5. Product matrix\n",
    "\n",
    "product information, or in this case, movie information. Each matrix takes the respective\n",
    "\n",
    "6. Factor matrix axes\n",
    "\n",
    "labeled axis from the original matrix, and is given another axis that is unlabeled. The unlabeled axes contain what’s called\n",
    "\n",
    "7. Latent feature axes\n",
    "\n",
    "latent features. The number of latent features is referred to as the \"rank\" of these matrices. In this case, the rank chosen is 3. You, as a data scientist get to choose how many of these ALS will create. These latent features represent groups that are created from patterns in the original ratings matrix and the values in these columns represent how much each item falls into these groups. For example,\n",
    "\n",
    "8. Horror vs drama\n",
    "\n",
    "in the original ratings matrix, there might be a lot of people who like horror movies and don’t like dramas. They would rate\n",
    "\n",
    "9. Horror vs Drama Part II\n",
    "\n",
    "horror movies high\n",
    "\n",
    "10. Horror vs Drama Part III\n",
    "\n",
    "and dramas low. Likewise other people might like dramas and not like horror movies, and would rate\n",
    "\n",
    "11. Horror vs Drama Part IV\n",
    "\n",
    "dramas high\n",
    "\n",
    "12. Horror vs Romance Part V\n",
    "\n",
    "and horror films low. ALS can see this and determine that these are different types of movies.\n",
    "\n",
    "13. Horror vs Drama Part VI\n",
    "\n",
    "And if we were to look at the movie factor matrix, we would likely see that in one of the latent feature rows the dramas would score high, and the horror movies would score low, while in another latent feature column we might see the\n",
    "\n",
    "14. Horror vs Drama Part VII\n",
    "\n",
    "opposite. Knowing a little about these movies, we could determine that those latent features reflect those two genres. This allows us to mathematically see how users experience these movies and to what degree users feel each movie falls into each respective category. This concept goes a bit deeper though. For example, if we were to look at a\n",
    "\n",
    "15. Uncovering Features\n",
    "\n",
    "movie matrix, we might see in one latent-feature column,\n",
    "\n",
    "16. Uncovering Features Part II\n",
    "\n",
    "that several movies have scored very high, but they\n",
    "\n",
    "17. Uncovering Features Part III\n",
    "\n",
    "don’t seem to have anything in common. If they are all popular movies,\n",
    "\n",
    "18. Uncovering Features Part IV\n",
    "\n",
    "we might want to research what’s going on here to see if there's a business opportunity. Digging deeper,\n",
    "\n",
    "19. Shakespeare Adaptations\n",
    "\n",
    "we find that these movies are all adaptations of Shakespeare plays, and that there seems to be a strong customer group that likes these types of movies. Now that we know this, we can use this information to inform how we choose what movies to make and hopefully give our customers more of what they want. It’s worth reiterating that in the original data set there was no column anywhere called “Shakespeare Adaptations”. It’s also worth noting that many or all of these customers may not even know that this is something that draws them to these movies. This is the type of powerful information that ALS can help us uncover.\n",
    "\n",
    "20. Let's practice!\n",
    "\n",
    "Now lets try and actually use this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7d3046-81da-4f53-9678-72861f06217f",
   "metadata": {},
   "source": [
    "### Alternate uses of recommendation engines.\n",
    "Select the best definition of \"latent features\".\n",
    "\n",
    "Answer the question\n",
    "\n",
    "- Features or tags that have manually been attached to items that categorize those items.\n",
    "\n",
    "- **Features that are contained in data, but aren't directly observable.**\n",
    "\n",
    "- Features that show up \"later\" in the machine learning process.\n",
    "\n",
    "- Features that are added by human beings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a4cdeb-c5f7-415a-9a0f-046cbcb807e2",
   "metadata": {},
   "source": [
    "### Confirm understanding of latent features\n",
    "Matrix `P` is provided here. Its columns represent movies and its rows represent several latent features. Use your understanding of Spark commands to view matrix `P` and see if you can determine what some of the latent features might represent. After examining the matrix, look at the dataframe `Pi`, which contains a rough approximation of what these latent features could represent. See if you weren't far off.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Examine matrix `P` using the `.show()` method.\n",
    "- Examine matrix `Pi` using the `.show()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f10017eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = spark.read.csv(\"MatrixP.csv\", inferSchema = True, header = True)\n",
    "Pi = spark.read.csv(\"MatrixPi.csv\", inferSchema = True, header= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55115829-4880-49d6-9f04-a6834adec95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------+---------+------------+------+----------+\n",
      "|Iron Man|Finding Nemo|Avengers|Toy Story|Forrest Gump|Wall-E|Green Mile|\n",
      "+--------+------------+--------+---------+------------+------+----------+\n",
      "|     0.2|         2.4|     0.1|      2.4|         0.0|   2.5|       0.0|\n",
      "|     1.5|         1.4|     1.4|      1.3|         1.8|   1.8|       2.5|\n",
      "|     2.5|         1.1|     2.4|      0.9|         0.2|   0.9|      0.09|\n",
      "|     1.9|         2.0|     1.5|      2.2|         1.2|   0.3|      0.01|\n",
      "|     0.0|         0.0|     0.0|      2.3|         2.2|   0.0|       2.5|\n",
      "+--------+------------+--------+---------+------------+------+----------+\n",
      "\n",
      "+---------+--------+------------+--------+---------+------------+------+----------+\n",
      "| Lat Feat|Iron Man|Finding Nemo|Avengers|Toy Story|Forrest Gump|Wall-E|Green Mile|\n",
      "+---------+--------+------------+--------+---------+------------+------+----------+\n",
      "| Animated|     0.2|         2.4|     0.1|      2.4|         0.0|   2.5|       0.0|\n",
      "|    Drama|     1.5|         1.4|     1.4|      1.3|         1.8|   1.8|       2.5|\n",
      "|Superhero|     2.5|         1.1|     2.4|      0.9|         0.2|   0.9|      0.09|\n",
      "|   Comedy|     1.9|         2.0|     1.5|      2.2|         1.2|   0.3|      0.01|\n",
      "|Tom Hanks|     0.0|         0.0|     0.0|      1.8|         2.2|   0.0|       2.5|\n",
      "+---------+--------+------------+--------+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Examine matrix P using the .show() method\n",
    "P.show()\n",
    "\n",
    "# Examine matrix Pi using the .show() method\n",
    "Pi.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290defef-a15d-47fc-bf31-3629070b229b",
   "metadata": {},
   "source": [
    "# How does ALS work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44265591-eb96-4465-8567-60397f53c221",
   "metadata": {},
   "source": [
    "## Overview of matrix multiplication\n",
    "1. Matrix Multiplication\n",
    "\n",
    "As you've probably realized, matrix operations are fundamental to the ALS algorithm. We're going to review matrix multiplication and matrix factorization here. Let's start with multiplication.\n",
    "\n",
    "2. Matrix Multiplication\n",
    "\n",
    "Here we have two square matrices. In order to multiply them together, we make specific pairs of the values from the two matrices, and add the products of those pairs. We start at the top left-hand corner of each matrix, and create pairs moving to the right on the first matrix, and moving down on the second matrix one at a time. Each pair is multiplied, and the products from all pairs are added together. The final sum will make up one number of the resulting matrix. That's a lot to digest, so let's walk through an example. Starting at the top\n",
    "\n",
    "3. Matrix Multiplication\n",
    "\n",
    "left number of each matrix we have a pair of numbers, 1 and 9. We will multiply those numbers together. Then moving to the right on the first matrix, and down on the second matrix we have\n",
    "\n",
    "4. Matrix Multiplication\n",
    "\n",
    "2 and 6, then moving\n",
    "\n",
    "5. Matrix Multiplication\n",
    "\n",
    "right again on the first matrix and down again on the second matrix we have 3 and 3. We have completed the first set of pairs, so let's add their products together. 1 times 9, plus 2 times 6 plus 3 times 3 is\n",
    "\n",
    "6. Matrix Multiplication\n",
    "\n",
    "9 plus 12 plus 9, which gives us\n",
    "\n",
    "7. Matrix Multiplication\n",
    "\n",
    "30. 30 is the first number in our final matrix. From here we stay on the first row of the first matrix, but move on to the second column of the\n",
    "\n",
    "8. Matrix Multiplication\n",
    "\n",
    "second matrix. These pairs give us 1 and 8,\n",
    "\n",
    "9. Matrix Multiplication\n",
    "\n",
    "2 and 5,\n",
    "\n",
    "10. Matrix Multiplication\n",
    "\n",
    "and 3 and 2. 1 times 8 plus 2 times 5 plus 3 times 2 is equal to 8 plus 10 plus 6, which is\n",
    "\n",
    "11. Matrix Multiplication\n",
    "\n",
    "24. Moving to the next set of pairs, we multiply\n",
    "\n",
    "12. Matrix Multiplication\n",
    "\n",
    "1 and 7, 2 and 4, and 3 and 1. Their products are 7, 8 and 3 which makes\n",
    "\n",
    "13. Matrix Multiplication\n",
    "\n",
    "18. Once we've multiplied the first row of the first matrix by all columns of the second matrix, we then go through the same process for\n",
    "\n",
    "14. Matrix Multiplication\n",
    "\n",
    "the second row of the first matrix with all the columns of the second matrix,\n",
    "\n",
    "15. Matrix Multiplication\n",
    "\n",
    "and so on\n",
    "\n",
    "16. Matrix Multiplication\n",
    "\n",
    "until all rows of the\n",
    "\n",
    "17. Matrix Multiplication\n",
    "\n",
    "first matrix have been multiplied\n",
    "\n",
    "18. Matrix Multiplication\n",
    "\n",
    "by all columns of the second matrix. In this example, we multiplied two square matrices of the same dimensions. In reality, you can multiply any two matrices as long as the\n",
    "\n",
    "19. Matrix Multiplication\n",
    "\n",
    "number of columns of the first matrix matches the number of rows of the second matrix,\n",
    "\n",
    "20. Matrix Multiplication\n",
    "\n",
    "If they don't, then some values in one of the matrixces won't be paired, and multiplication can't be completed.\n",
    "\n",
    "21. Let's practice!\n",
    "\n",
    "Let's look at some examples, and practice matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215c5e85-a164-4959-b489-5c40908fc1f6",
   "metadata": {},
   "source": [
    "### Matrix multiplication\n",
    "To understand matrix multiplication more directly, let's do some matrix operations manually.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Matrices `a` and `b` are Pandas dataframes. Use the `.head()` method on each of them to view them.\n",
    "- Work out the product of these two matrices on your own.\n",
    "- Enter the values of the product of the `a` and `b` matrices into the product array, created using `np.array()`.\n",
    "- Use the validation on the last line of code to evaluate your estimate. The `.dot()` method multiplies two matrices together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17530c3f-be06-41db-a658-af13c275c94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "one  2  2\n",
       "two  3  3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.DataFrame(columns = [0,1], index= ['one', 'two'], data = np.array([[2,2], [3,3]]))\n",
    "b = pd.DataFrame(columns = [0,1], index= ['one', 'two'], data = np.array([[1,2], [4,4]]))\n",
    "a.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c514e89-ec6e-4c3a-ae63-b8a232f31db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "one  1  2\n",
       "two  4  4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f4d7787-f676-4cca-b155-dcf87798031d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A: \n",
      "     0  1\n",
      "one  2  2\n",
      "two  3  3\n",
      "Matrix B: \n",
      "     0  1\n",
      "one  1  2\n",
      "two  4  4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ True,  True],\n",
       "       [ True,  True]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the .head() method to view the contents of matrices a and b\n",
    "print(\"Matrix A: \")\n",
    "print (a.head())\n",
    "\n",
    "print(\"Matrix B: \")\n",
    "print (b.head())\n",
    "\n",
    "# Complete the matrix with the product of matrices a and b\n",
    "product = np.array([[10,12], [15,18]])\n",
    "\n",
    "# Run this validation to see how your estimate performs\n",
    "product == np.dot(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d58079b-6372-4fcc-9400-d22e315f4207",
   "metadata": {},
   "source": [
    "### Matrix multiplication part II\n",
    "Let's put your matrix multiplication skills to the test.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Use the `.shape` attribute to understand the dimensions of matrices `C` and `D`, and determine whether these two matrices can be multiplied together or not.\n",
    "- If they can be multiplied, use the `np.matmul()` method to multiply them. If not, set `C_times_D` to `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b2cf4e9-cb8f-4193-a772-354554582768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4\n",
       "0  3  4  5  1  2\n",
       "1  2  5  7  6  8\n",
       "2  1  9  0  7  6\n",
       "3  2  2  3  3  1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = pd.DataFrame(data = np.array([[3 , 4 , 5 , 1,  2], [2 , 5  ,7  ,6  ,8], [1,  9  ,0  ,7  ,6], [2 , 2  ,3 , 3 ,1]])) \n",
    "D = pd.DataFrame(data = np.array([[1, 2], [3, 3],[9,8]]))\n",
    "C.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "272dede4-5890-4b0c-bf69-3cfb162baa08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  1  2\n",
       "1  3  3\n",
       "2  9  8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc9e4231-9b50-4a66-9e29-fc5ea5195552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 5)\n",
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "# Print the dimensions of C\n",
    "print(C.shape)\n",
    "\n",
    "# Print the dimensions of D\n",
    "print(D.shape)\n",
    "\n",
    "# Can C and D be multiplied together?\n",
    "C_times_D = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1fe9a8-af2e-43d0-9137-bce366c755b9",
   "metadata": {},
   "source": [
    "That's right. The number of columns in `C` is different than the number of rows in `D`. `C` and `D` cannot be multiplied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff94a04-eb1b-4235-8957-e71f8859dc46",
   "metadata": {},
   "source": [
    "## Overview of matrix factorization\n",
    "1. Overview of matrix factorization\n",
    "\n",
    "Matrix factorization, or matrix decomposition, is essentially the opposite of matrix multiplication. Rather than multiplying two matrices together to get one new matrix, matrix factorization\n",
    "\n",
    "2. Matrix Factorization\n",
    "\n",
    "splits a matrix into two or more matrices which, when multiplied back together,\n",
    "\n",
    "3. Matrix Factorization\n",
    "\n",
    "produce an approximation of the original matrix. There are several different mathematical approaches for this, each of which has a different application. We aren't going to go into any of that here, we are simply going to review the factorization that ALS performs. Used in the context of collaborative filtering, ALS uses a factorization called non-negative matrix factorization. Because matrix factorization generally returns only approximations of the original matrix, in some cases, they can return negative values in the factor matrices, even when attempting to predict positive values. When predicting what rating a user will give to an item, negative values don't really make sense. Neither do they make sense in the context of latent features. For this reason, the version of ALS that we will use will require that the factorization return only positive values. Let's look at some sample factorizations.\n",
    "\n",
    "4. Matrix Factorization\n",
    "\n",
    "Here is a sample matrix of possible item ratings. There are 5 rows and 5 columns. And here\n",
    "\n",
    "5. Matrix Factorization\n",
    "\n",
    "is one factorization of that matrix called the LU factorization. Notice that the factor matrices are the same dimensions or rank as the original matrix. Also notice that some of the values in this factorization are negative. Using this type of factorization could result in negative predictions that wouldn't make sense in our context.\n",
    "\n",
    "6. Matrix Factorization\n",
    "\n",
    "Here is another factorization. In this case, all the values are positive meaning the resulting product of these factor matrices is guaranteed to be positive. This is closer to what we need for our purposes. Notice here that the dimensions of the factor matrices are such that the first factor matrix has the same number of rows as the original matrix, but a different number of columns. Also, the second factor matrix has the same number of columns as the original matrix, but a different number of rows. The dimensions of the factor matrices that don't match the original matrix are called the \"rank\" or number of latent features. In this case, we have chosen the \"rank\" of the factor matrices to be 3. What that means is that the number of\n",
    "\n",
    "7. Rank of Factor Matrices\n",
    "\n",
    "latent features of the factor matrices is 3. Remember that as a data scientist, when doing these types of factorizations, you get to choose the rank, or number of latent features the factor matrices will have.\n",
    "\n",
    "8. Filling in the Blanks\n",
    "\n",
    "Now look at this matrix. Not all the cells have numbers in them. Despite this, we can still\n",
    "\n",
    "9. Filling in the Blanks\n",
    "\n",
    "factor the values in the matrix. Also notice that because there is at least\n",
    "\n",
    "10. Filling In the Blanks\n",
    "\n",
    "one value in\n",
    "\n",
    "11. Filling In the Blanks\n",
    "\n",
    "every row and at\n",
    "\n",
    "12. Filling In the Blanks\n",
    "\n",
    "least one value in every column\n",
    "\n",
    "13. Filling In the Blanks\n",
    "\n",
    "that each of the factor matrices are totally full. Because of this, factoring a sparse matrix into two factor matrices gives us the means to not only approximate the original values that existed in the matrix to begin with, but to also\n",
    "\n",
    "14. Filling In the Blanks\n",
    "\n",
    "provide predictions for the cells that were originally blank. And because the factorization is based on the values that existed previously, the blank cells are filled in based on those already-existing patterns. So when we do this with user ratings, the blanks are filled in with values that reflect the individual user behavior and the behavior of users similar to them. This is why this method is called collaborative filtering.\n",
    "\n",
    "15. Let's practice!\n",
    "\n",
    "Let's look at some real-life examples of this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da38eedc",
   "metadata": {},
   "source": [
    "### Matrix factorization\n",
    "Matrix `G` is provided here as a Pandas dataframe. View it to understand what it looks like. Look at the possible factor matrices `H`, `I`, and `J` (also Pandas dataframes), and determine which two matrices will produce the matrix `G` when multiplied together.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Take a look at matrix `G` using the print command\n",
    "- Take a look at the matrices `H`, `I`, and `J` and determine which pair of those matrices will produce `G` when multiplied together.\n",
    "- Input your answer into the `np.matmul()` code provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cc644f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  6  6\n",
       "1  3  3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = pd.DataFrame(data = np.array([[6, 6], [3, 3]]))\n",
    "H = pd.DataFrame(data = np.array([[2, 2], [1, 1]]))\n",
    "I = pd.DataFrame(data = np.array([[3, 3], [3, 3]]))\n",
    "J = pd.DataFrame(data = np.array([[1, 1], [2, 2]]))\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81d3601c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  2  2\n",
       "1  1  1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ecf24db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  3  3\n",
       "1  3  3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16367046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  1  1\n",
       "1  2  2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8c6e120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix G:\n",
      "   0  1\n",
      "0  6  6\n",
      "1  3  3\n",
      "Matrix H:\n",
      "   0  1\n",
      "0  2  2\n",
      "1  1  1\n",
      "Matrix I:\n",
      "   0  1\n",
      "0  3  3\n",
      "1  3  3\n",
      "Matrix J:\n",
      "   0  1\n",
      "0  1  1\n",
      "1  2  2\n",
      "      0     1\n",
      "0  True  True\n",
      "1  True  True\n"
     ]
    }
   ],
   "source": [
    "# Take a look at Matrix G using the following print function\n",
    "print(\"Matrix G:\")\n",
    "print(G)\n",
    "\n",
    "# Take a look at the matrices H, I, and J and determine which pair of those matrices will produce G when multiplied together. \n",
    "print(\"Matrix H:\")\n",
    "print(H)\n",
    "print(\"Matrix I:\")\n",
    "print(I)\n",
    "print(\"Matrix J:\")\n",
    "print(J)\n",
    "\n",
    "# Multiply the two matrices that are factors of the matrix G\n",
    "prod = np.matmul(H, J)\n",
    "print(G == prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad8fad7",
   "metadata": {},
   "source": [
    "### Non-negative matrix factorization\n",
    "It's possible for one matrix to have two equally close factorizations where one has all positive values and the other has some negative values.\n",
    "\n",
    "The matrix `M` has been factored twice using two different factorizations. Take a look at each pair of factor matrices `L` and `U`, and `W` and `H` to see the differences. Then use their products to see that they produce essentially the same product.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Use `print()` to view the `L` and `U` matrices. Notice that some values in matrices `L` and `U` are negative.\n",
    "- Use `print()` to view the `W` and `H` matrices. Notice that all values in these two matrices are positive.\n",
    "- The `L` and `U` matrices and `W` and `H` matrices have been multiplied together to produce the `LU` and `WH` matrices respectively. Use `getRMSE`(`product_matrix`, `original_matrix`) to see how close `LU` is to `M` compared to how close `WH` is to `M`. Are they similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8183b5",
   "metadata": {},
   "source": [
    "To get the source for getRMSE function I used:\n",
    "\n",
    "`import inspect`\n",
    "\n",
    "`lines = inspect.getsource(getRMSE)`\n",
    "\n",
    "`print(lines)`\n",
    "\n",
    "More details [here](https://stackoverflow.com/questions/427453/how-can-i-get-the-source-code-of-a-python-function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "325a0464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRMSE(pred, actual):\n",
    "    \"\"\"\n",
    "    Returns RMSE between predictions and actual observations\n",
    "    \n",
    "    Parameters: \n",
    "        predictions: pandas dataframe of value predictions\n",
    "        actual values: pandas dataframe of actual values that predictions are trying to predict\n",
    "        \n",
    "    Returns: RMSE value in decimal format\n",
    "    \"\"\"\n",
    "    RMSE =  (((pred - actual)**2).sum().sum()/(pred.shape[0]*pred.shape[1]))**.5\n",
    "    return round(RMSE,3)\n",
    "\n",
    "M = pd.DataFrame(data = np.array([ [1,  2 , 1,  2], [  0 , 0 , 0 , 0] , [  1 , 2 , 2 , 1], [  0 , 0 , 0 , 0] ]))\n",
    "\n",
    "L = pd.DataFrame(data = np.array([[1.00 , 0.000000 , 0.000000 , 0],[  0.01, -0.421053 , 0.098316 , 1], [ 1.00  ,0.000000 , 1.000000 , 0], [0.10 , 1.000000 , 0.000000 , 0]]))\n",
    "U = pd.DataFrame(data = np.array([   [ 1 , 2.00 , 1.000 , 2.000000],  [0 ,-0.19 ,-0.099 ,-0.198000],  [0 , 0.00 , 1.000 ,-1.000000],  [0 , 0.00,  0.000,  0.194947]]))\n",
    "\n",
    "W = pd.DataFrame(data = np.array([   [ 2.61 , 0.24 , 0.00 , 0.12],  [0.00,  0.05,  0.02,  0.17], [ 1.97 , 0.00 , 0.58 , 0.83], [ 0.05 , 0.00 , 0.00 , 0.00]]))\n",
    "H = pd.DataFrame(data= np.array([    [0.38 , 0.65,  0.34 , 0.41],  [0.00,  1.20 , 0.15  ,3.72],  [0.42 , 1.09 , 1.38 , 0.07],  [0.00 , 0.11 , 0.65 , 0.17]]))\n",
    "\n",
    "LU = np.matmul(L,U)\n",
    "WH = np.matmul(W,H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47c1730a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrices L and U:\n",
      "      0         1         2    3\n",
      "0  1.00  0.000000  0.000000  0.0\n",
      "1  0.01 -0.421053  0.098316  1.0\n",
      "2  1.00  0.000000  1.000000  0.0\n",
      "3  0.10  1.000000  0.000000  0.0\n",
      "     0     1      2         3\n",
      "0  1.0  2.00  1.000  2.000000\n",
      "1  0.0 -0.19 -0.099 -0.198000\n",
      "2  0.0  0.00  1.000 -1.000000\n",
      "3  0.0  0.00  0.000  0.194947\n",
      "Matrices W and H:\n",
      "      0     1     2     3\n",
      "0  2.61  0.24  0.00  0.12\n",
      "1  0.00  0.05  0.02  0.17\n",
      "2  1.97  0.00  0.58  0.83\n",
      "3  0.05  0.00  0.00  0.00\n",
      "      0     1     2     3\n",
      "0  0.38  0.65  0.34  0.41\n",
      "1  0.00  1.20  0.15  3.72\n",
      "2  0.42  1.09  1.38  0.07\n",
      "3  0.00  0.11  0.65  0.17\n",
      "RMSE of LU:  0.072\n",
      "RMSE of WH:  0.071\n"
     ]
    }
   ],
   "source": [
    "# View the L, U, W, and H matrices.\n",
    "print(\"Matrices L and U:\") \n",
    "print(L)\n",
    "print(U)\n",
    "\n",
    "print(\"Matrices W and H:\")\n",
    "print(W)\n",
    "print(H)\n",
    "\n",
    "# Calculate RMSE between LU and M\n",
    "print(\"RMSE of LU: \", getRMSE(LU, M))\n",
    "\n",
    "# Calculate RMSE between WH and M\n",
    "print(\"RMSE of WH: \", getRMSE(WH, M))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4231145b-1462-4479-9cda-cde88be2ab06",
   "metadata": {},
   "source": [
    "## Data preparation for Spark ALS\n",
    "1. Data preparation for Spark ALS\n",
    "\n",
    "Let's talk about data preparation. Data preparation will consist of two things: 1. Correct dataframe format 2. Correct schema First, dataframe format.\n",
    "\n",
    "2. Conventional Dataframe\n",
    "\n",
    "Most dataframes you've seen probably look like this, with userId's in one column, all the features in the remaining columns, and the values of those features making up the contents of those columns. However, many Pyspark algorithms, ALS included, require your data to be in row-based format\n",
    "\n",
    "3. Row-based data format\n",
    "\n",
    "like this. The data is the same. The first column contains userIds, but rather than a different feature in each column, column 2 contains feature names, and column 3 contains the value of that feature for that user. So a user's data can be spread\n",
    "\n",
    "4. Row-based data format (cont.)\n",
    "\n",
    "across several rows, and rows contain no null values. Depending on your data, you may need to convert it to this format. Now let's talk about creating the right schema.\n",
    "\n",
    "5. Correct schema\n",
    "\n",
    "As you see, our userId column and our generically named column of movie titles are strings. Pyspark's implementation of ALS can only consume\n",
    "\n",
    "6. Must be integers\n",
    "\n",
    "userIds and movieIds as integers.So, again, you might need to convert your data to integers. Let's walk through an example of how to do all of this.\n",
    "\n",
    "7. Conventional Dataframe\n",
    "\n",
    "Here's a conventional dataframe. To convert it to a \"long\" or \"dense\" matrix, we will use a user-defined function called \"wide_to_long\":\n",
    "\n",
    "8. Wide to long function\n",
    "\n",
    "We won't go into the detail of how it works here, but it turns the conventional dataframe into a row-based dataframe like this:\n",
    "\n",
    "9. Long DF Output\n",
    "\n",
    "If you'd like to access this function directly, a link will be provided at the end of the course. So we have the right dataframe format, let's get the right schema. In order to have integer user and movieId's we need to assign unique integers to the userId's and the movieId's. To do this, we will follow 3 steps\n",
    "\n",
    "10. Steps to get integer ID's\n",
    "\n",
    "1. Extract unique userIds/movieIds 2. Assign unique integers to each id 3. Rejoin these unique integer id's back to the ratings data. Let's start with userIds.\n",
    "\n",
    "11. Extracting distinct user IDs\n",
    "\n",
    "Let's first run this query to get all the distinct userIds into one dataframe and call it users.\n",
    "\n",
    "12. Monotonically increasing ID\n",
    "\n",
    "Then we'll import a method called \"monotonically_increasing_id()\" which will assign a unique integer to each row of our users dataframe. We need to be careful when using this because it will treat each partition of data independently, meaning the same integer could be used in different partitions. In order to get around this, we'll convert our data into one partition using the coalesce method.\n",
    "\n",
    "13. Coalesce method\n",
    "\n",
    "Also note that while the integers will be increasing by a value of 1 over each row, they may not necessarily start at 1. That's not super important here, what's really important is that they are unique. So now we can create a new column in our users dataframe\n",
    "\n",
    "14. Persist method\n",
    "\n",
    "called userIntId, set it to monotonicallyIncreasingId, and we will have our new userIntegerIds. Note that the monotonically_increasing_id() method can be a bit tricky as the values it provides can change as you do different things to your dataset. For this reason, we've called the .persist() method to tell Spark to keep these values the same across all dataframe operations. We'll do the\n",
    "\n",
    "15. Movie integer IDs\n",
    "\n",
    "same thing with the movie id's and now we have two dataframes, one with our userIds and one with our movieIds. So let's join\n",
    "\n",
    "16. Joining UserIds and MovieIds\n",
    "\n",
    "them together along with our original dataframe on our userId and variable columns using the .join() method, specifying a \"left\" join. We can be even more thorough by creating a new dataframe with only the columns ALS needs, and renaming our columns using the .alias() method, which renames the column on which it is called. Like this:\n",
    "\n",
    "17. Joining User and Movie Integer Ids\n",
    "\n",
    "like this.\n",
    "\n",
    "18. Let's practice!\n",
    "\n",
    "Now let's prepare some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c918aa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = pd.DataFrame( columns = ['U_LF_1',  'U_LF_2' , 'U_LF_3' , 'U_LF_4'], index = ['User_1', 'User_2' , 'User_3', 'User_4', 'User_5','User_6','User_7','User_8','User_9'], data = np.array([\n",
    "    [0.80  ,  0.01 ,   0.30  ,   0.8],\n",
    "[    0.40  ,  0.01 ,   0.06   ,  0.2]  , \n",
    "    [0.05  ,  2.10 ,   0.01  ,   2.2],\n",
    "   [ 0.30  ,  0.01 ,   0.20  ,   0.2],\n",
    "   [ 0.10  ,  1.50  ,  0.90 ,    0.0],\n",
    "    [0.00  ,  0.03 ,   0.40  ,   0.5],\n",
    "    [0.01  ,  0.02 ,   0.66  ,   0.4],\n",
    "    [0.90  ,  0.70  ,  0.00  ,   1.0],\n",
    "    [1.00  ,  2.00  ,  0.04  ,   0.2]]))\n",
    "P =   pd.DataFrame(columns = ['Movie_1' , 'Movie_2' , 'Movie_3' , 'Movie_4'], index = ['P_LF_1', 'P_LF_2', 'P_LF_3', 'P_LF_4'], data = np.array([\n",
    "[        0.5 ,     0.1 ,     0.4  ,   1.10],\n",
    "[      0.2   ,   2.0  ,    0.0   ,  0.01],\n",
    "[      0.3   ,   1.9   ,   0.6  ,   0.90],\n",
    "[      1.0    ,  0.2  ,    1.0 ,    0.89]  ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eb7f12",
   "metadata": {},
   "source": [
    "### Estimating recommendations\n",
    "Use your knowledge of matrix multiplication to determine which movie will have the highest recommendation for User_3. The ratings matrix has been factorized into `U` and `P` with ALS.\n",
    "\n",
    "**Instructions 1/3**\n",
    "\n",
    "- View the left factor matrix, `U`, using the `print` function.\n",
    "\n",
    "**Instructions 2/3 **\n",
    "\n",
    "- Did you see anything interesting about `User_3`? Now inspect the right factor matrix, `P`. Use the print function.\n",
    "\n",
    "**Instructions 3/3**\n",
    "\n",
    "- Looking at `U` and `P`, which movie do you think will have the highest recommendation for `User_3`.\n",
    "- Multiply `U` and `P` using numpy's matmul to obtain recommendations. Call this `UP`.\n",
    "- Complete the code to print `UP`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21d3b667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        U_LF_1  U_LF_2  U_LF_3  U_LF_4\n",
      "User_1    0.80    0.01    0.30     0.8\n",
      "User_2    0.40    0.01    0.06     0.2\n",
      "User_3    0.05    2.10    0.01     2.2\n",
      "User_4    0.30    0.01    0.20     0.2\n",
      "User_5    0.10    1.50    0.90     0.0\n",
      "User_6    0.00    0.03    0.40     0.5\n",
      "User_7    0.01    0.02    0.66     0.4\n",
      "User_8    0.90    0.70    0.00     1.0\n",
      "User_9    1.00    2.00    0.04     0.2\n",
      "        Movie_1  Movie_2  Movie_3  Movie_4\n",
      "P_LF_1      0.5      0.1      0.4     1.10\n",
      "P_LF_2      0.2      2.0      0.0     0.01\n",
      "P_LF_3      0.3      1.9      0.6     0.90\n",
      "P_LF_4      1.0      0.2      1.0     0.89\n",
      "        Movie_1  Movie_2  Movie_3  Movie_4\n",
      "User_1      NaN      NaN      NaN      NaN\n",
      "User_2      NaN      NaN      NaN      NaN\n",
      "User_3      NaN      NaN      NaN      NaN\n",
      "User_4      NaN      NaN      NaN      NaN\n",
      "User_5      NaN      NaN      NaN      NaN\n",
      "User_6      NaN      NaN      NaN      NaN\n",
      "User_7      NaN      NaN      NaN      NaN\n",
      "User_8      NaN      NaN      NaN      NaN\n",
      "User_9      NaN      NaN      NaN      NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmad.alashmony\\AppData\\Local\\Temp\\ipykernel_16068\\635015382.py:8: FutureWarning: Calling a ufunc on non-aligned DataFrames (or DataFrame/Series combination). Currently, the indices are ignored and the result takes the index/columns of the first DataFrame. In the future , the DataFrames/Series will be aligned before applying the ufunc.\n",
      "Convert one of the arguments to a NumPy array (eg 'ufunc(df1, np.asarray(df2)') to keep the current behaviour, or align manually (eg 'df1, df2 = df1.align(df2)') before passing to the ufunc to obtain the future behaviour and silence this warning.\n",
      "  UP = np.matmul(U,P)\n"
     ]
    }
   ],
   "source": [
    "# View left factor matrix 1/3\n",
    "print(U)\n",
    "\n",
    "# View right factor matrix 2/3\n",
    "print(P)\n",
    "\n",
    "# Multiply factor matrices 3/3\n",
    "UP = np.matmul(U,P)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "print(pd.DataFrame(data = UP, columns = P.columns, index = U.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a56002c",
   "metadata": {},
   "source": [
    "### RMSE as ALS alternates\n",
    "As you know, ALS will alternate between the two factor matrices, adjusting their values each time to iteratively come closer and closer to approximating the original ratings matrix. This exercise is intended to illustrate this to you.\n",
    "\n",
    "Matrix `T` is a ratings matrix, and matrices `F1`, `F2`, `F3`, `F4`, `F5`, and `F6` are the respective products of ALS after iterating 2, 3, 4, 5, and 6 times respectively. Follow the instructions below to see how the RMSE changes as ALS iterates.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Use `getRMSE(pred_matrix, actual_matrix)` to calculate the RMSE of `F1`.\n",
    "- Put `F2`, `F3`, `F4`, `F5`, and `F6` into one list called `Fs`.\n",
    "- Complete the `getRMSEs(listOfPredMatrices, actualValues)` function to calculate the RMSE of each matrix in the `Fs` list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb454f8",
   "metadata": {},
   "source": [
    "#### fs\n",
    "- F1\n",
    "\n",
    "   0  1  2  3\n",
    "0  2  4  3  3\n",
    "1  1  3  2  1\n",
    "2  1  4  4  3\n",
    "3  1  4  4  3\n",
    "4  3  3  1  3\n",
    "5  4  2  4  1\n",
    "6  2  4  3  4\n",
    "7  4  3  3  4\n",
    "8  4  1  3  2\n",
    "\n",
    "- F2\n",
    "\n",
    "          0         1         2         3\n",
    "0  0.727685  0.000000  0.781614  0.000000\n",
    "1  0.297216  0.000000  0.289833  0.149972\n",
    "2  0.668410  5.133314  2.090948  1.727615\n",
    "3  0.103563  0.449466  0.000000  0.000000\n",
    "4  0.278805  0.000000  0.564327  0.326220\n",
    "5  0.183318  0.000000  0.000000  0.229848\n",
    "6  0.000000  1.817581  0.718101  0.687327\n",
    "7  0.981968  2.720611  1.029219  0.000000\n",
    "8  0.000000  2.667367  0.820015  0.000000\n",
    "\n",
    "- F3\n",
    "\n",
    "          0         1         2         3\n",
    "0  1.492245  0.000000  0.990605  0.000000\n",
    "1  0.397179  0.000000  0.435733  0.563462\n",
    "2  0.141205  4.940441  1.881492  1.905120\n",
    "3  0.109256  0.343033  0.000000  0.000000\n",
    "4  0.619273  0.000000  0.686859  0.973293\n",
    "5  0.377921  0.000000  0.000000  0.608201\n",
    "6  0.000000  1.350676  0.676210  0.883204\n",
    "7  1.391171  1.810945  1.390281  0.000000\n",
    "8  0.000000  3.820890  1.160296  0.000000\n",
    "\n",
    "- F4\n",
    "\n",
    "          0         1         2         3\n",
    "0  1.232812  0.000000  0.963432  0.000000\n",
    "1  0.308675  0.000000  0.406568  0.827733\n",
    "2  0.399142  4.868625  1.987510  1.673421\n",
    "3  0.281845  0.419753  0.000000  0.000000\n",
    "4  0.531455  0.000000  0.626833  1.098208\n",
    "5  0.319177  0.000000  0.000000  0.855895\n",
    "6  0.000000  1.488357  0.719290  0.807520\n",
    "7  1.615384  1.553621  1.526890  0.000000\n",
    "8  0.000000  4.018753  0.856822  0.000000\n",
    "\n",
    "- F5\n",
    "\n",
    "          0         1         2         3\n",
    "0  1.191828  0.000000  0.756902  0.000000\n",
    "1  0.482960  0.000000  0.466946  0.634913\n",
    "2  0.270822  4.665225  2.179591  2.058923\n",
    "3  0.364895  0.466308  0.000000  0.000000\n",
    "4  0.703867  0.000000  0.661523  0.896099\n",
    "5  0.829117  0.000000  0.000000  0.413693\n",
    "6  0.000000  1.350148  0.788875  1.027619\n",
    "7  1.476283  1.632972  1.457697  0.000000\n",
    "8  0.000000  4.086277  0.735722  0.000000\n",
    "\n",
    "- F6\n",
    "\n",
    "          0         1         2         3\n",
    "0  1.392534  0.000000  1.190954  0.000000\n",
    "1  0.448433  0.000000  0.388912  0.654163\n",
    "2  0.086018  4.628878  2.205928  2.051924\n",
    "3  0.248835  0.437081  0.000000  0.000000\n",
    "4  0.659538  0.000000  0.570960  0.909904\n",
    "5  0.611369  0.000000  0.000000  0.815595\n",
    "6  0.000000  1.338655  0.859938  0.944789\n",
    "7  1.510931  1.681587  1.445032  0.000000\n",
    "8  0.000000  4.214108  0.629327  0.000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2784eea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRMSE(pred, actual):\n",
    "    \"\"\" \n",
    "    Compute the RMSE between pred and actual values.\n",
    "    \n",
    "    Parameters:\n",
    "      - pred: list or array of predictions\n",
    "      - actual: list or array of true values\n",
    "      \n",
    "    Returns: RMSE\n",
    "    \"\"\"\n",
    "    MSE = (((pred - actual)**(2)).sum().sum()/(pred.shape[0]*pred.shape[1]))\n",
    "    print (\"F1: \", MSE**.5)\n",
    "    \n",
    "def getRMSEs(listOfPredMatrices, actualValues):\n",
    "    \"\"\" \n",
    "    Computes the RMSE between predictions and actual values for various sets of predictions.\n",
    "    \n",
    "    Parameters:\n",
    "      - pred: list of matrices each containing predictions\n",
    "      - actual: array of true values\n",
    "      \n",
    "    Returns: Printed statement of integer index of predition and respective RMSE based on actual values provided.\n",
    "    \"\"\"\n",
    "    for i, pred in enumerate(listOfPredMatrices):\n",
    "        MSE = (((pred - actualValues)**(2)).sum().sum()/(pred.shape[0]*pred.shape[1]))\n",
    "        print (\"F\" + str(i+2) + \":\", MSE**.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16edfc54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Use getRMSE(preds, actuals) to calculate the RMSE of matrices T and F1.\\ngetRMSE(F1, T)\\n\\n# Create list of F2, F3, F4, F5, and F6\\nFs = [F2, F3, F4, F5, F6]\\n\\n# Calculate RMSE for F2, F3, F4, F5, and F6.\\ngetRMSEs(Fs, T)\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Use getRMSE(preds, actuals) to calculate the RMSE of matrices T and F1.\n",
    "getRMSE(F1, T)\n",
    "\n",
    "# Create list of F2, F3, F4, F5, and F6\n",
    "Fs = [F2, F3, F4, F5, F6]\n",
    "\n",
    "# Calculate RMSE for F2, F3, F4, F5, and F6.\n",
    "getRMSEs(Fs, T)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf6a13b-cd49-43e1-bbc1-f80a87899b2d",
   "metadata": {},
   "source": [
    "## ALS parameters and hyperparameters\n",
    "1. ALS parameters and hyperparameters\n",
    "\n",
    "As with most algorithms, ALS has arguments that we give it and hyperparameters which must be tuned in order to generate the best predictions.\n",
    "\n",
    "2. Example ALS model code\n",
    "\n",
    "Here is what a built-out ALS model looks like. Let's review each argument and hyperparameter.\n",
    "\n",
    "3. Column names\n",
    "\n",
    "The userCol, itemCol and ratingCol are straightforward. They simply tell spark which columns in your dataframe contain the respective userIds', itemIds' and ratings. The first ALS hyperparameter is the rank.\n",
    "\n",
    "4. Rank\n",
    "\n",
    "As you already know, ALS will take a matrix of ratings, and it will factor that matrix into two different matrices, one representing the users, and the other representing the products, or items, or in our case, movies. In the process of doing this,\n",
    "\n",
    "5. Rank (cont.)\n",
    "\n",
    "latent features are uncovered. ALS allows you to choose the number of latent features that are created, which is referred to as the \"rank\" hyperparameter, often represented by the letter k.\n",
    "\n",
    "6. Rank\n",
    "\n",
    "Your objective with the data will be to determine the \"rank\". If you're trying to find meaningful groupings or categories of movies to see how similar or different movies are, you may want to experiment with different numbers of latent features. If you have too few or too many latent features, the groupings might be difficult to understand, so you'll want to look at different numbers of latent features and manually identify what makes the most sense. For purposes of recommendations however, the best number of latent features will be found through cross-validation.\n",
    "\n",
    "7. MaxIter\n",
    "\n",
    "The number of iterations, or \"maxIter\" simply tells ALS how many times to iterate back and forth between the factors matrices, adjusting the values to reduce the RMSE. Obviously the higher number of iterations, the longer it will take to complete, and the fewer number of iterations, the higher the risk of not fully reducing the error. So you'll have to determine what works you.\n",
    "\n",
    "8. RegParam\n",
    "\n",
    "Many other machine learning algorithms have a regularization parameter, often called lambda. A lambda is simply a number that is added to an error metric to keep the algorithm from converging too quickly and overfitting to the training data. The lambda for ALS in Pyspark is referred to as the \"regParam\".\n",
    "\n",
    "9. Alpha\n",
    "\n",
    "We'll talk about alpha later in the course, but suffice it to say that alpha is only used when using implicit ratings, and not used with explicit ratings.\n",
    "\n",
    "10. Non-negative\n",
    "\n",
    "Let's talk about the ALS arguments. As mentioned previously, there are several different factorizations that can be used to factor a matrix. The one that we are interested in is the non-negative factorization, so we set the nonnegative argument to True.\n",
    "\n",
    "11. Cold start strategy\n",
    "\n",
    "You might be familiar with the term coldStartStrategy already. In the context of ALS, when splitting data into test and train sets, it's possible for a user to have all of their ratings inadvertantly put into the test set, leaving nothing in the train set to be used for making a prediction. In this case, ALS can't make meaningful predictions for that user, or calculate an error metric. To avoid this, we set the coldStartStrategy to \"drop\" which tells Spark that when these situations arise, to not use them to calculate the RMSE, and to only use users that have ratings in both the test AND training set.\n",
    "\n",
    "12. Implicit preferences\n",
    "\n",
    "We also need to tell Spark whether our ratings are implicit or explicit. We do this by setting the implicitPrefs argument to True or False.\n",
    "\n",
    "13. Sample ALS model build\n",
    "\n",
    "Once we have a built-out model like you see here, we can fit it to training data, and then generate test predictions to see how well it performs. We can do this by\n",
    "\n",
    "14. Fit and transform methods\n",
    "\n",
    "calling the fit and transform methods as you see here. You'll do this yourself in subsequent exercises.\n",
    "\n",
    "15. Let's practice!\n",
    "\n",
    "Now it's your turn to build some models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c81915b",
   "metadata": {},
   "source": [
    "### Correct format and distinct users\n",
    "Take a look at the `R` dataframe. Notice that it is in conventional or \"wide\" format with a different movie in each column. Also notice that the User's and movie names are not in integer format. Follow the steps to properly prepare this data for ALS.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import the `monotonically_increasing_id` package from `pyspark.sql.functions` and view the `R` dataframe using the `.show()` method.\n",
    "- Use the `to_long()` function to convert the `R` dataframe into a \"long\" data frame. Call the new dataframe `ratings`.\n",
    "- Create a dataframe called users that contains all the `.distinct()` users from the dataframe and repartition the dataframe into one partition using the `.coalesce(1)` method.\n",
    "- Use the `monotonically_increasing_id()` method inside of `withColumn()` to create a new column in the users dataframe that contains a unique integer for each user. Call this column `userId`. Be sure to call the `.persist()` method on the final dataframe to ensure the new integer IDs persist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ba011a",
   "metadata": {},
   "source": [
    "R DataFrame\n",
    "\n",
    "|            User|Shrek|Coco|Swing Kids|Sneakers|\n",
    "|----------------|-----|----|----------|--------|\n",
    "|    James Alking|    3|   4|         4|       3|\n",
    "|Elvira Marroquin|    4|   5|      null|       2|\n",
    "|      Jack Bauer| null|   2|         2|       5|\n",
    "|     Julia James|    5|null|         2|       2|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c915e0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#R = pd.DataFrame(columns = ['User','Shrek','Coco','Swing' 'Kids','Sneakers'], data = np.array([\n",
    "#['James Alking',3,4,4,3],['Elvira Marroquin',4,5,np.nan,2],['Jack Bauer',np.nan,2,2,5],['Julia James',5,np.nan,2,2]]))\n",
    "\n",
    "#R= spark.createDataFrame(R)\n",
    "\n",
    "R= spark.read.csv(\"to_long_dataset.csv\", inferSchema=True, header=True)\n",
    "\n",
    "from pyspark.sql.functions import explode, lit, struct, col\n",
    "from numpy import array\n",
    "\n",
    "def to_long(df, by = [\"User\"]):\n",
    "    \"\"\" \n",
    "    Converts traditional or \"wide\" dataframe into a \"row-based\" dataframe, also known as a \"dense\" or \"long\" dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "      - df: array of columns with column names\n",
    "      - by: name of column which serves as\n",
    "      \n",
    "    Returns: Row-based dataframe with no null values\n",
    "    \"\"\"\n",
    "    cols = [c for c in df.columns if c not in by]\n",
    "    # Create and explode an array of (column_name, column_value) structs\n",
    "    kvs = explode(array([\n",
    "      struct(lit(c).alias(\"Movie\"), col(c).alias(\"Rating\")) for c in cols\n",
    "    ])).alias(\"kvs\")\n",
    "    return df.select(by + [kvs]).select(by + [\"kvs.Movie\", \"kvs.Rating\"]).filter(\"rating IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68176ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Import monotonically_increasing_id and show R\\nfrom pyspark.sql.functions import monotonically_increasing_id\\nR.show()\\n\\n# Use the to_long() function to convert the dataframe to the \"long\" format.\\nratings = to_long(R)\\nratings.show()\\n\\n# Get unique users and repartition to 1 partition\\nusers = ratings.select(\"User\").distinct().coalesce(1)\\n\\n# Create a new column of unique integers called \"userId\" in the users dataframe.\\nusers = users.withColumn(\"userId\", monotonically_increasing_id()).persist()\\nusers.show()\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Import monotonically_increasing_id and show R\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "R.show()\n",
    "\n",
    "# Use the to_long() function to convert the dataframe to the \"long\" format.\n",
    "ratings = to_long(R)\n",
    "ratings.show()\n",
    "\n",
    "# Get unique users and repartition to 1 partition\n",
    "users = ratings.select(\"User\").distinct().coalesce(1)\n",
    "\n",
    "# Create a new column of unique integers called \"userId\" in the users dataframe.\n",
    "users = users.withColumn(\"userId\", monotonically_increasing_id()).persist()\n",
    "users.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56adc16e",
   "metadata": {},
   "source": [
    "### Assigning integer id's to movies\n",
    "Let's do the same thing to the movies. Then let's join the new user IDs and movie IDs into one dataframe.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "\n",
    "- Use the `.select()` and the `.distinct()` methods to extract all unique Movies from the ratings dataframe.\n",
    "- Repartition the movies dataframe to one partition using `coalesce()`.\n",
    "- Complete the partial code provided to assign unique integer IDs to each movie. Name the new column movieId and call the `.persist()` method on the resulting dataframe.\n",
    "- Join the ratings dataframe to the users dataframe and subsequently to the movies dataframe. Call the result `movie_ratings`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59da0810",
   "metadata": {},
   "source": [
    "#### Ratings\n",
    "|            User|     Movie|Rating|\n",
    "|----------------|----------|------|\n",
    "|    James Alking|     Shrek|     3|\n",
    "|    James Alking|      Coco|     4|\n",
    "|    James Alking|Swing Kids|     4|\n",
    "|    James Alking|  Sneakers|     3|\n",
    "|Elvira Marroquin|     Shrek|     4|\n",
    "|Elvira Marroquin|      Coco|     5|\n",
    "|Elvira Marroquin|  Sneakers|     2|\n",
    "|      Jack Bauer|      Coco|     2|\n",
    "|      Jack Bauer|Swing Kids|     2|\n",
    "|      Jack Bauer|  Sneakers|     5|\n",
    "|     Julia James|     Shrek|     5|\n",
    "|     Julia James|Swing Kids|     2|\n",
    "|     Julia James|  Sneakers|     2|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7421ac7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Extract the distinct movie id\\'s\\nmovies = ratings.select(\"Movie\").distinct() \\n\\n# Repartition the data to have only one partition.\\nmovies = movies.coalesce(1) \\n\\n# Create a new column of movieId integers. \\nmovies = movies.withColumn(\"movieId\", monotonically_increasing_id()).persist() \\n\\n# Join the ratings, users and movies dataframes\\nmovie_ratings = ratings.join(users, \"User\", \"left\").join(movies, \"Movie\", \"left\")\\nmovie_ratings.show()\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Extract the distinct movie id's\n",
    "movies = ratings.select(\"Movie\").distinct() \n",
    "\n",
    "# Repartition the data to have only one partition.\n",
    "movies = movies.coalesce(1) \n",
    "\n",
    "# Create a new column of movieId integers. \n",
    "movies = movies.withColumn(\"movieId\", monotonically_increasing_id()).persist() \n",
    "\n",
    "# Join the ratings, users and movies dataframes\n",
    "movie_ratings = ratings.join(users, \"User\", \"left\").join(movies, \"Movie\", \"left\")\n",
    "movie_ratings.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b5136c-e240-46af-90fd-48085309a9d5",
   "metadata": {},
   "source": [
    "# Recommending Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38d32ba-9b8a-4375-83bc-b2daa5af9222",
   "metadata": {},
   "source": [
    "## Introduction to the MovieLens dataset\n",
    "1. Introduction to the MovieLens dataset\n",
    "\n",
    "Up until now we've only been using sample datasets. Now we're going to begin using actual data using the\n",
    "\n",
    "2. MovieLens dataset\n",
    "\n",
    "MovieLens dataset. This dataset is made available by the good people at GroupLens.org and contains\n",
    "\n",
    "3. MovieLens summary stats\n",
    "\n",
    "roughly 20 million ratings for over 138,000 users and more than 27,000 movies. In order to provide you with a better learning experience, we will achieve shorter runtimes by using a subset of the original dataset including 100,000 ratings. In addition to the ratings data, Grouplens.org also provides additional datafiles that include information on movie genres and other types of tags that movie watchers have provided for them. We'll take what you've learned from the previous chapters and explore the data, prepare the data, build out a cross-validated ALS model, generate predictions and assess the model's performance. First we'll view the data using the\n",
    "\n",
    "4. Explore the data\n",
    "\n",
    ".show() and .columns() methods, as well as some other methods to understand the nature of the dataset.\n",
    "\n",
    "5. MovieLens sparsity\n",
    "\n",
    "Then we'll calculate it's sparsity using this sparsity formula, and then we'll assess whether further preparation is needed in order to adequately prepare it for ALS. If you're not familiar with the term sparsity, it simply provides a measure of how empty a matrix is, or what percentage of the matrix is empty. In essence, this formula is simply the number of ratings that a matrix contains divided by the number of ratings it could contain given the number of users and movies in the matrix.\n",
    "\n",
    "6. Sparsity: numerator\n",
    "\n",
    "The code to calculate sparsity is pretty straightforward. We'll simply get the numerator by counting the number of ratings in the ratings dataframe\n",
    "\n",
    "7. Sparsity: users and movies\n",
    "\n",
    "then we'll get the number of distinct users and the number of distinct items or movies.\n",
    "\n",
    "8. Sparsity: denominator\n",
    "\n",
    "We'll then multiply the number of users and number of movies together to get the denominator\n",
    "\n",
    "9. Sparsity\n",
    "\n",
    "and simply divide the numerator by the denominator, and substract the result from 1. Because division in Pyhton will return an integer, we multiply the numerator by 1.0 to ensure a decimal or float is returned. Let's go over some other techniques that may or may not be new to you.\n",
    "\n",
    "10. The .distinct() method\n",
    "\n",
    "As you may already know, the .distinct() method simply returns all the unique values in a column. For example, if you want to know how many unique users there are in a table, you could simply select the userId column from the dataframe, then run the distinct and count methods like you see here.\n",
    "\n",
    "11. GroupBy method\n",
    "\n",
    "The groupBy method organizes data by the unique values of a specific column to return subtotals for those unique values. For example{{1}}, if you wanted to look at total number of ratings each user has provided you would first need to groupBy userId as you see here, then\n",
    "\n",
    "12. GroupBy method\n",
    "\n",
    "call the count method as you see here. With this, you could then\n",
    "\n",
    "13. GroupBy method min\n",
    "\n",
    "get the min\n",
    "\n",
    "14. GroupBy method max\n",
    "\n",
    "or max\n",
    "\n",
    "15. GroupBy method avg\n",
    "\n",
    "or average of that same column.\n",
    "\n",
    "16. Filter method\n",
    "\n",
    "The filter method allows you to filter out any data that doesn't meet your specified criteria. {{1}}For example if you wanted to only consider users that have rated at least 20 movies, you would simply apply the same groupby and count methods, and then add a filter method specifying that the count column should only include values greater than 20.\n",
    "\n",
    "17. Let's practice!\n",
    "\n",
    "Let's apply what you've learned to a real data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eec739",
   "metadata": {},
   "source": [
    "### Build out an ALS model\n",
    "Let's specify your first ALS model. Complete the code below to build your first ALS model.\n",
    "\n",
    "Recall that you can use the `.columns` method on the ratings data frame to see what the names of the columns are that contain user, movie, and ratings data. Spark needs to know the names of these columns in order to perform ALS correctly.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Before building our ALS model, we need to split the data into training data and test data. Use the `randomSplit()` method to split the ratings dataframe into `training_data` and `test_data` using an 0.8/0.2 split respectively and a seed for the random number generator of `42`.\n",
    "- Tell Spark which columns contain the `userCol`, `itemCol` and `ratingCol`. Use the `.columns` method if needed. Complete the hyperparameters. Set the `rank` to 10, the `maxIter` to 15, the `regParam` or lambda to .1, the `coldStartStrategy` to \"drop\", the `nonnegative` argument should be set to True, and since our data contains explicit ratings, set the `implicitPrefs` argument to False.\n",
    "- Now fit the als model to the `training_data` portion of the ratings data by calling the `als.fit()` method on the `training_data` provided. Call the fitted model model.\n",
    "- Generate predictions on the `test_data` portion of the ratings data by calling the `model.transform()` method on the `test_data` provided. Call the predictions `test_predictions`. Feel free to view the predictions by calling the `.show()` method on the `test_predictions`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7404beed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = spark.read.csv('toy_ratings.csv', inferSchema=True, header=True)\n",
    "ratings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9adbdab7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.20.1)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating|prediction|\n",
      "+------+-------+------+----------+\n",
      "|     2|      0|     3| 1.7681035|\n",
      "|     2|      2|     4| 1.2946719|\n",
      "|     0|      3|     4| 3.8376758|\n",
      "+------+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the ratings dataframe into training and test data\n",
    "(training_data, test_data) = ratings.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Set the ALS hyperparameters\n",
    "from pyspark.ml.recommendation import ALS\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", rank =10, maxIter =15, regParam =0.1,\n",
    "          coldStartStrategy=\"drop\", nonnegative =True, implicitPrefs = False)\n",
    "\n",
    "# Fit the mdoel to the training_data\n",
    "model = als.fit(training_data)\n",
    "\n",
    "# Generate predictions on the test_data\n",
    "test_predictions = model.transform(test_data)\n",
    "test_predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977702fa",
   "metadata": {},
   "source": [
    "### Build RMSE evaluator\n",
    "Now that you know how to fit a model to training data and generate test predictions, you need a way to evaluate how well your model performs. For this we'll build an evaluator. Evaluators in Spark can be built out in various ways. For our purposes, we want a `regressionEvaluator` that calculates the RMSE. After we build our `regressionEvaluator`, we can fit the model to our data and generate predictions.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import the required `RegressionEvaluator` package from the `pyspark.ml.evaluation` class.\n",
    "- Complete the evaluator code, specifying the `metric` name to be \"rmse\". Set the `labelCol` to the name of the column in our ratings data that contains our ratings (use the `ratings.columns` method to see column names) and set the `prediction` column name to \"prediction\".\n",
    "- Confirm that the evaluator was properly created by extracting each of the three parameters from it. Do this by running the following 3 lines of code, each within a print statement:\n",
    "    - `evaluator.getMetricName()`\n",
    "    - `evaluator.getLabelCol()`\n",
    "    - `evaluator.getPredictionCol()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fff8b2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse\n",
      "rating\n",
      "prediction\n"
     ]
    }
   ],
   "source": [
    "# Import RegressionEvaluator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Complete the evaluator code\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "# Extract the 3 parameters\n",
    "print(evaluator.getMetricName())\n",
    "print(evaluator.getLabelCol())\n",
    "print(evaluator.getPredictionCol())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d843baa8",
   "metadata": {},
   "source": [
    "### Get RMSE\n",
    "Now that you know how to build a model and generate predictions, and have an evaluator to tell us how well it predicts ratings, we can calculate the RMSE to see how well an ALS model performed. We'll use the evaluator that we built in the previous exercise to calculate and print the rmse.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Call the `.evaluate()` method on our evaluator to calculate our RMSE on the `test_predictions` dataframe. Call the result `RMSE`.\n",
    "- Print the `RMSE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f894322b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7187901116515298\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the \"test_predictions\" dataframe\n",
    "RMSE = evaluator.evaluate(test_predictions)\n",
    "\n",
    "# Print the RMSE\n",
    "print (RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db46c20-8c5f-4cb0-b0f8-af0d604a6520",
   "metadata": {},
   "source": [
    "## ALS model buildout on MovieLens Data\n",
    "1. ALS model buildout on MovieLens Data\n",
    "\n",
    "If you remember from the last chapter, you built out a model on the ratings dataset. The code looked like this:\n",
    "\n",
    "2. Fitting a basic model\n",
    "\n",
    "Now, the RMSE that you got was lower than the 1.45 shown here. But what if you went through this whole process and got an error metric that you weren't satisfied with, like this RMSE of 1.45. You might want to try other combinations of hyperparameter values to try and reduce that. Spark makes it easy to do this by using two additional tools called\n",
    "\n",
    "3. Intro to ParamGridBuilder and CrossValidator\n",
    "\n",
    "the ParamGridBuilder and the CrossValidator. These tools will allow you to try many different hyperparameter values and have Spark identify the best combination. Let's talk about how to use them.\n",
    "\n",
    "4. ParamGridBuilder\n",
    "\n",
    "The ParamGridBuilder tells Spark all the hyperparameter values you want it to try. To do this, we first import the ParamGridBuilder package, instantiate it and give it a name. We'll call it param_grid. We then add each hyperparameter name calling the .addGrid()\n",
    "\n",
    "5. Adding Hyperparameters to the ParamGridBuilder\n",
    "\n",
    "method on our als algorithm and hyperparameter name as you see here. Notice the empty lists to the right of the hyperparameter names. This is where we input the values we want Spark to try for each hyperparameter, like this:\n",
    "\n",
    "6. Adding Hyperparameter Values to the ParamGridBuilder\n",
    "\n",
    "Once we've added all of this, we call the .build() method to complete the build of our param_grid. Now let's look at the CrossValidator.\n",
    "\n",
    "7. CrossValidator\n",
    "\n",
    "The CrossValidator essentially fits a model to several different portions of our training dataset called folds, and then generates predictions for each respective holdout portion of the dataset to see how it performs.\n",
    "\n",
    "8. CrossValidator instantiation and estimator\n",
    "\n",
    "To properly use the CrossValidator, we first import the CrossValidator package, instantiate a CrossValidator and give it a name, we'll call it cv here.\n",
    "\n",
    "9. CrossValidator ParamMaps\n",
    "\n",
    "We then tell it to use our als model as an estimator by setting estimator argument equal to the name of our model which is als. We'll set the estimatorParamMaps to our param_grid that we built so that Spark knows what values to try as it works to identify the best combination of hyperparameters. Then we provide the name of our evaluator so it knows how to measure each model's performance by simply setting the evaluator argument to the name of our evaluator which is \"evaluator\".\n",
    "\n",
    "10. CrossValidator\n",
    "\n",
    "We finish by setting the numFolds argument to the number of times we want Spark to test each model on the training data, in this case, 5 times. Let's go over how to integrate these into a full code buildout.\n",
    "\n",
    "11. Random split\n",
    "\n",
    "We'll first split our data into training and test sets using the randomSplit() method and we'll build a generic ALS model without any hyperparameters, only the model parameters as you see here. The cross validator will take care of the hyperparameters.\n",
    "\n",
    "12. ParamGridBuilder\n",
    "\n",
    "We'll build our ParamGridBuilder so Spark knows what hyperparameter values to test.\n",
    "\n",
    "13. Evaluator\n",
    "\n",
    "We'll create an evaluator so Spark knows how to evaluate each model.\n",
    "\n",
    "14. CrossValidator\n",
    "\n",
    "Then the CrossValidator will tell Spark the algorithm, the hyperparameters and values, and the evaluator to use to find the best model, and the number of training set folds we want each model to be tested on.\n",
    "\n",
    "15. Best model\n",
    "\n",
    "We then fit our CrossValidator on the training data to have Spark try all the combindations of hyperparameters we specified by calling the cv.fit() method on the training data. Once it's finished running, we extract the best-performing model by calling the bestModel() method on our model. We'll call this our best_model and\n",
    "\n",
    "16. Predictions and performance evaluation\n",
    "\n",
    "with it, we can generate predictions on the test set, print the error metric and the respective hyperparameter values using the code you see here. And now we have our cross-validated model.\n",
    "\n",
    "17. Let's practice!\n",
    "\n",
    "Let's build a real model on a real dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059e7441",
   "metadata": {},
   "source": [
    "### Viewing the MovieLens Data\n",
    "Familiarize yourself with the ratings dataset provided here. Would you consider the data to be implicit or explicit ratings?\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Look at the `.columns` of the ratings dataframe.\n",
    "- Look at the first few rows of ratings dataframe using the `.show()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71c3c334",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = spark.read.csv('ratings.csv', inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7bf26384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['userId', 'movieId', 'rating', 'timestamp']\n",
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|     31|   2.5|1260759144|\n",
      "|     1|   1029|   3.0|1260759179|\n",
      "|     1|   1061|   3.0|1260759182|\n",
      "|     1|   1129|   2.0|1260759185|\n",
      "|     1|   1172|   4.0|1260759205|\n",
      "|     1|   1263|   2.0|1260759151|\n",
      "|     1|   1287|   2.0|1260759187|\n",
      "|     1|   1293|   2.0|1260759148|\n",
      "|     1|   1339|   3.5|1260759125|\n",
      "|     1|   1343|   2.0|1260759131|\n",
      "|     1|   1371|   2.5|1260759135|\n",
      "|     1|   1405|   1.0|1260759203|\n",
      "|     1|   1953|   4.0|1260759191|\n",
      "|     1|   2105|   4.0|1260759139|\n",
      "|     1|   2150|   3.0|1260759194|\n",
      "|     1|   2193|   2.0|1260759198|\n",
      "|     1|   2294|   2.0|1260759108|\n",
      "|     1|   2455|   2.5|1260759113|\n",
      "|     1|   2968|   1.0|1260759200|\n",
      "|     1|   3671|   3.0|1260759117|\n",
      "+------+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Look at the column names\n",
    "print(ratings.columns)\n",
    "\n",
    "# Look at the first few rows of data\n",
    "print(ratings.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c193167",
   "metadata": {},
   "source": [
    "### Calculate sparsity\n",
    "As you know, ALS works well with sparse datasets. Let's see how much of the ratings matrix is actually empty.\n",
    "\n",
    "Remember that sparsity is calculated by the number of cells in a matrix that contain a rating divided by the total number of values that matrix could hold given the number of users and items (movies). In other words, dividing the number of ratings present in the matrix by the product of users and movies in the matrix and subtracting that from 1 will give us the sparsity or the percentage of the `ratings` matrix that is empty.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Calculate the `numerator` of the sparsity metric by counting the total number of ratings that are contained in the ratings matrix.\n",
    "- Calculate the number of `distinct()` `userIds` and `distinct()` `movieIds` in the ratings matrix.\n",
    "- Calculate the denominator of the sparsity metric by multiplying the number of users by the number of movies in the ratings matrix.\n",
    "- Calculate and print the sparsity by dividing the `numerator` by the `denominator`, subtracting from 1 and multiplying by 100. The 1.0 is added to ensure the `sparsity` is returned as a decimal and not an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dfae46d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratings dataframe is  98.36% empty.\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of ratings in the dataset\n",
    "numerator = ratings.select(\"rating\").count()\n",
    "\n",
    "# Count the number of distinct userIds and distinct movieIds\n",
    "num_users = ratings.select(\"userId\").distinct().count()\n",
    "num_movies = ratings.select(\"movieId\").distinct().count()\n",
    "\n",
    "# Set the denominator equal to the number of users multiplied by the number of movies\n",
    "denominator = num_movies * num_users\n",
    "\n",
    "# Divide the numerator by the denominator\n",
    "sparsity = (1.0 - (numerator *1.0)/denominator)*100\n",
    "print(\"The ratings dataframe is \", \"%.2f\" % sparsity + \"% empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d644474a",
   "metadata": {},
   "source": [
    "### The GroupBy and Filter methods\n",
    "Now that we know a little more about the dataset, let's look at some general summary metrics of the ratings dataset and see how many ratings the movies have and how many ratings each users has provided.\n",
    "\n",
    "Two common methods that will be helpful to you as you aggregate summary statistics in Spark are the `.filter()` and the `.groupBy()` methods. The `.filter()` method allows you to filter out any data that doesn't meet your specified criteria.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import `col` from the `pyspark.sql.functions`, and view the ratings dataset using the `.show()`.\n",
    "- Apply the `.filter()` method on the ratings dataset with the following filter inside the parenthesis in order to include only `userId`'s less than 100: `col(\"userId\") < 100`.\n",
    "- Call the `.groupBy()` method on the ratings dataset to group the data by `userId`. Call the `.count()` method to see how many movies each `userId` has rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90fa8d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|     31|   2.5|1260759144|\n",
      "|     1|   1029|   3.0|1260759179|\n",
      "|     1|   1061|   3.0|1260759182|\n",
      "|     1|   1129|   2.0|1260759185|\n",
      "|     1|   1172|   4.0|1260759205|\n",
      "|     1|   1263|   2.0|1260759151|\n",
      "|     1|   1287|   2.0|1260759187|\n",
      "|     1|   1293|   2.0|1260759148|\n",
      "|     1|   1339|   3.5|1260759125|\n",
      "|     1|   1343|   2.0|1260759131|\n",
      "|     1|   1371|   2.5|1260759135|\n",
      "|     1|   1405|   1.0|1260759203|\n",
      "|     1|   1953|   4.0|1260759191|\n",
      "|     1|   2105|   4.0|1260759139|\n",
      "|     1|   2150|   3.0|1260759194|\n",
      "|     1|   2193|   2.0|1260759198|\n",
      "|     1|   2294|   2.0|1260759108|\n",
      "|     1|   2455|   2.5|1260759113|\n",
      "|     1|   2968|   1.0|1260759200|\n",
      "|     1|   3671|   3.0|1260759117|\n",
      "+------+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|     31|   2.5|1260759144|\n",
      "|     1|   1029|   3.0|1260759179|\n",
      "|     1|   1061|   3.0|1260759182|\n",
      "|     1|   1129|   2.0|1260759185|\n",
      "|     1|   1172|   4.0|1260759205|\n",
      "|     1|   1263|   2.0|1260759151|\n",
      "|     1|   1287|   2.0|1260759187|\n",
      "|     1|   1293|   2.0|1260759148|\n",
      "|     1|   1339|   3.5|1260759125|\n",
      "|     1|   1343|   2.0|1260759131|\n",
      "|     1|   1371|   2.5|1260759135|\n",
      "|     1|   1405|   1.0|1260759203|\n",
      "|     1|   1953|   4.0|1260759191|\n",
      "|     1|   2105|   4.0|1260759139|\n",
      "|     1|   2150|   3.0|1260759194|\n",
      "|     1|   2193|   2.0|1260759198|\n",
      "|     1|   2294|   2.0|1260759108|\n",
      "|     1|   2455|   2.5|1260759113|\n",
      "|     1|   2968|   1.0|1260759200|\n",
      "|     1|   3671|   3.0|1260759117|\n",
      "+------+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+-----+\n",
      "|userId|count|\n",
      "+------+-----+\n",
      "|   148|  132|\n",
      "|   463|  483|\n",
      "|   471|  216|\n",
      "|   496|  126|\n",
      "|   243|  307|\n",
      "|   392|   25|\n",
      "|   540|   20|\n",
      "|   623|  103|\n",
      "|    31|   69|\n",
      "|   516|  149|\n",
      "|    85|  107|\n",
      "|   137|   80|\n",
      "|   251|  119|\n",
      "|   451|   52|\n",
      "|   580|  922|\n",
      "|    65|   27|\n",
      "|   458|   76|\n",
      "|    53|   46|\n",
      "|   255|  145|\n",
      "|   481|  436|\n",
      "+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the requisite packages\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# View the ratings dataset\n",
    "ratings.show()\n",
    "\n",
    "# Filter to show only userIds less than 100\n",
    "ratings.filter(col(\"userId\") < 100).show()\n",
    "\n",
    "# Group data by userId, count ratings\n",
    "ratings.groupBy(\"userId\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb7b9dd",
   "metadata": {},
   "source": [
    "### MovieLens Summary Statistics\n",
    "Let's take the `groupBy()` method a bit further.\n",
    "\n",
    "Once you've applied the `.groupBy()` method to a dataframe, you can subsequently run aggregate functions such as `.sum()`, `.avg()`, `.min()` and have the results grouped. This exercise will walk you through how this is done. The `min` and `avg` functions have been imported from `pyspark.sql.functions` for you.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Group the data by `movieId` and use the `.count()` method to calculate how many ratings each movie has received. From there, call the `.select()` method to select the following metrics:\n",
    "    - `min(\"count\")` to get the smallest number of ratings that any movie in the dataset. This first one is given to you as an example.\n",
    "    - `avg(\"count\")` to get the average number of ratings per movie\n",
    "Do the same thing, but this time group by userId to get the min and avg number of ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "759d3163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "322e1122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie with the fewest ratings: \n",
      "+----------+\n",
      "|min(count)|\n",
      "+----------+\n",
      "|         1|\n",
      "+----------+\n",
      "\n",
      "Avg num ratings per movie: \n",
      "+------------------+\n",
      "|        avg(count)|\n",
      "+------------------+\n",
      "|11.030664019413193|\n",
      "+------------------+\n",
      "\n",
      "User with the fewest ratings: \n",
      "+----------+\n",
      "|min(count)|\n",
      "+----------+\n",
      "|        20|\n",
      "+----------+\n",
      "\n",
      "Avg num ratings per user: \n",
      "+------------------+\n",
      "|        avg(count)|\n",
      "+------------------+\n",
      "|149.03725782414307|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Min num ratings for movies\n",
    "print(\"Movie with the fewest ratings: \")\n",
    "ratings.groupBy(\"movieId\").count().select(min(\"count\")).show()\n",
    "\n",
    "# Avg num ratings per movie\n",
    "print(\"Avg num ratings per movie: \")\n",
    "ratings.groupBy(\"movieId\").count().select(avg(\"count\")).show()\n",
    "\n",
    "# Min num ratings for user\n",
    "print(\"User with the fewest ratings: \")\n",
    "ratings.groupBy(\"userId\").count().select(min(\"count\")).show()\n",
    "\n",
    "# Avg num ratings per users\n",
    "print(\"Avg num ratings per user: \")\n",
    "ratings.groupBy(\"userId\").count().select(avg(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32016948",
   "metadata": {},
   "source": [
    "#### View Schema\n",
    "As you know from previous chapters, Spark's implementation of ALS requires that `movieIds` and `userIds` be provided as `integer` datatypes. Many datasets need to be prepared accordingly in order for them to function properly with Spark. A common issue is that Spark thinks numbers are strings, and vice versa.\n",
    "\n",
    "Here, you'll use the `.cast()` method to address these types of problems. Let's take a look at the schema of the dataset to ensure it's in the correct format.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Use `.printSchema()` to check whether the ratings dataset contains the proper data types for ALS to function correctly. Are the `userIds` and `movieIds` provided as integer datatypes? Are the ratings in numeric format?\n",
    "- Ensure that the columns of the ratings dataframe are the correct data types. Call the `cast()` method on each column and specify the `userID` and `movieId` columns to be type \"integer\" and the rating column to be of type \"double\". (We don't need the timestamp column, so we can leave that out.)\n",
    "- Call `.printSchema()` again on `ratings` to confirm that the data types are now correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ac62fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use .printSchema() to see the datatypes of the ratings dataset\n",
    "ratings.printSchema()\n",
    "\n",
    "# Tell Spark to convert the columns to the proper data types\n",
    "ratings = ratings.select(ratings.userId.cast(\"integer\"), ratings.movieId.cast(\"integer\"), ratings.rating.cast(\"double\"))\n",
    "\n",
    "# Call .printSchema() again to confirm the columns are now in the correct format\n",
    "ratings.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0b0415-f8e1-4309-902a-a58bcb2073a0",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation\n",
    "1. Model Performance Evaluation and Output Cleanup\n",
    "\n",
    "Congratulations. You just built your first cross-validated ALS model. Now let's determine whether the model suits your needs or not. The primary way to do this is to examine\n",
    "\n",
    "2. Root mean squared error\n",
    "\n",
    "the error metric, in this case, the RMSE. The RMSE tells us, on average, how far a given prediction is from it's corresponding actual value.\n",
    "\n",
    "3. Pred vs actual\n",
    "\n",
    "It's fairly straightforward. If we have predictions and actual values,\n",
    "\n",
    "4. Pred vs actual: difference\n",
    "\n",
    "the RMSE subtracts she actual value from the prediction,\n",
    "\n",
    "5. Difference squared\n",
    "\n",
    "then squares those differences to make them positive.\n",
    "\n",
    "6. Sum of difference squared\n",
    "\n",
    "It then sums those differences,\n",
    "\n",
    "7. Average of difference squared\n",
    "\n",
    "takes the average by dividing by the number of observations, in this case N = 4,\n",
    "\n",
    "8. RMSE\n",
    "\n",
    "and it then takes the square root to undo the squaring of the values that we did previously. So if we have an RMSE of .61, then on average, our predictions are either .61 above or below the original rating. Another way to evaluate our model is to look at it's recommendations. Remember, however, that ALS is often used to identify patterns and uncover latent features that are unobservable by humans, meaning that ALS can sometimes see things that may not initially make sense to us as humans. Bear this in mind as you move forward. To generate recommendations, we will use the native Spark function\n",
    "\n",
    "9. Recommend for all users\n",
    "\n",
    "recommendForAllUsers() which generates the top recommendations for all users. ALS recommendation output has two challenges that need to be addressed. The first is that it is in a format like\n",
    "\n",
    "10. Unclean recommendation output\n",
    "\n",
    "this which is perfectly usable in Pyspark, but isn't very human-readable. To resolve this, we save the dataframe as a temporary table and use\n",
    "\n",
    "11. Cleaning up recommendation output\n",
    "\n",
    "this sql query to make it readable. The explode command essentially takes an array like our recommendation column and separates each item within it, like this:\n",
    "\n",
    "12. Explode function\n",
    "\n",
    "Notice that only one movieId and it's respective recommendation value for each user is contained on each line, where previously, all recommendations for a given user were contained on one line. Also notice that ALS conveniently includes the movieId and rating column names with each value on each line. This makes it easy to separate them into different columns.\n",
    "\n",
    "13. Adding lateral view\n",
    "\n",
    "Adding the LATERAL VIEW to the explode function allows us to treat the exploded column as a table, and extract the individual values as separate columns. We first name the lateral view, in this case we call it exploded_table and then give it a formal table name which we call movieIds_and_ratings. This allows us to SELECT userId, and then get the movieId and ratings by referencing the movieIds_and_ratings table in the beginning of our query. The output is now readable:\n",
    "\n",
    "14. Explode and lateral view together\n",
    "\n",
    "And if we join it to the original movie information,\n",
    "\n",
    "15. Joining clean recs with movie info\n",
    "\n",
    "we can see what's going on even better. The other challenge with these recommendations is that they include predictions for movies that have been already been watched. Remember how ALS creates two factor matrices that are multiplied together to produce an approximation of the original ratings matrix. That's essentially what the ALS output is, including all movies for all users, whether they've seen them or not. A simple way to address this is to filter out the movies that have already been seen. Since we already have our clean recommendations, and the original movie_ratings,\n",
    "\n",
    "16. Filtering recommendations\n",
    "\n",
    "we can simply join these two dataframes together on \"userId\" and \"movieId\" using a \"left\" join.\n",
    "\n",
    "17. Filtering recommendations (cont.)\n",
    "\n",
    "The movies that have already been seen are those that have a rating from the original movie_ratings dataframe,\n",
    "\n",
    "18. Filtering recommendations (cont.)\n",
    "\n",
    "so if we simply add a filter so that the \"rating\" column of the movie_ratings dataframe is null, we'll only have predictions for movies that the individual users haven't seen.\n",
    "\n",
    "19. Let's practice!\n",
    "\n",
    "Now let's evaluate your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb3909c",
   "metadata": {},
   "source": [
    "### Create test/train splits and build your ALS model\n",
    "You already know how to build an ALS model, having done it in the previous chapter. We will do that again, but we'll take some additional steps to fully build out a cross-validated model.\n",
    "\n",
    "First, let's import the requisite functions and create our train and test data sets in preparation for the cross validation step.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import the `RegressionEvaluator` from `ml.evaluation`, the ALS algorithm from `ml.recommendation`, and the `ParamGridBuilder` and the `CrossValidator` from `ml.tuning`.\n",
    "- Create an .80/.20 train/test split on the ratings data using the `randomSplit` method. Name the datasets `train` and `test`, and set the random seed to `1234`.\n",
    "- Build out the ALS model, telling Spark the names of the columns in the ratings dataframe that correspond to the `userCol`, `itemCol` and `ratingCol`. Set the `nonnegative` argument to `True`, the `coldStartStrategy` to `\"drop\"` and let Spark know that these are not `implicitPrefs` by setting the `implicitPrefs` argument to `False`. Call this model `als`.\n",
    "- Verify that the model was created by calling the `type()` function on `als`. The output should indicate what type of model it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f0488f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.recommendation.ALS"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the required functions\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create test and train set\n",
    "(train, test) = ratings.randomSplit([0.8, 0.2], seed = 1234)\n",
    "\n",
    "# Create ALS model\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", nonnegative = True, implicitPrefs = False, coldStartStrategy = \"drop\")\n",
    "\n",
    "# Confirm that a model called \"als\" was created\n",
    "type(als)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d1af78",
   "metadata": {},
   "source": [
    "### Tell Spark how to tune your ALS model\n",
    "Now we'll need to create a ParamGrid to tell Spark what hyperparameters we want it to tune, how to tune them, and then build out an evaluator so Spark can know how to measure the algorithm's performance.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import `RegressionEvaluator` from `pyspark.ml.evaluation` and `ParamGridBuilder` and `CrossValidator` from `pyspark.ml.tuning`.\n",
    "- Build a `ParamGrid` called `param_grid` using the `ParamGridBuilder` provided. Call the `.addGrid()` method on each hyperparameter by providing the name of the model and the name of each hyperparameter (ex: `.addGrid(als.rank, [])`. Do this for the `rank`, `maxIter` and `regParam` hyperparameters. Also provide the respective lists of hyperparameter values that Spark should try, as provided here:\n",
    "    - `rank: [10, 50, 100, 150]`\n",
    "    - `maxIter: [5, 50, 100, 200]`\n",
    "    - `regParam: [.01, .05, .1, .15]`\n",
    "- Create a `RegressionEvaluator` called `evaluator`. Set the `metricName` to `\"rmse\"`, set the `labelCol` to `\"rating\"`, and tell Spark that when it generates predictions to call the `predictionCol` `\"prediction\"`.\n",
    "- Run `len(param_grid)` to confirm that the `param_grid` was created and to confirm that the right number of hyperparameter combinations will be tested. It should be equal to the number of `rank` values * the number of `maxIter` values * the number of `regParam` values in the `ParamGridBuilder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "027a15f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num models to be tested:  64\n"
     ]
    }
   ],
   "source": [
    "# Import the requisite items\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Add hyperparameters and their respective values to param_grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(als.rank, [10, 50, 100, 150]  ) \\\n",
    "            .addGrid(als.maxIter, [5, 50, 100, 200]  ) \\\n",
    "            .addGrid(als.regParam,  [.01, .05, .1, .15]  ) \\\n",
    "            .build()\n",
    "           \n",
    "# Define evaluator as RMSE and print length of evaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\") \n",
    "print (\"Num models to be tested: \", len(param_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f69a68f",
   "metadata": {},
   "source": [
    "### Build your cross validation pipeline\n",
    "Now that we have our data, our train/test splits, our model, and our hyperparameter values, let's tell Spark how to cross validate our model so it can find the best combination of hyperparameters and return it to us.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Create a `CrossValidator` called `cv` with our `als` model as the estimator, setting `estimatorParamMaps` to the `param_grid` you just built. Tell Spark that the `evaluator` to be used is the `\"evaluator\"` we built previously. Set the `numFolds` to 5.\n",
    "- Confirm that our `cv` was built by printing `cv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "df94d463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossValidator_eba568950944\n"
     ]
    }
   ],
   "source": [
    "# Build cross validation using CrossValidator\n",
    "cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Confirm cv was built\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e847a06",
   "metadata": {},
   "source": [
    "### Best Model and Best Model Parameters\n",
    "Now that we have our cross validator, `cv`, built out, we can tell Spark to take our data, fit the ALS algorithm to it, and try the different combinations of hyperparameter values from our `param_grid` so that it can identify what values provide the smallest RMSE. Unfortunately, this takes too long to complete here, but for your reference, this is how it is done:\n",
    "\n",
    "```\n",
    "#Fit cross validator to the 'train' dataset\n",
    "model = cv.fit(train)\n",
    "\n",
    "#Extract best model from the cv model above\n",
    "best_model = model.bestModel\n",
    "```\n",
    "\n",
    "This code has been run separately, and the `best_model` has been identified and saved for you to use. Use the commands given to extract the parameters of the model.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Print type(`best_model`) to confirm that the model ALS built from our hyperparameter options is indeed completed. A print statement is needed here in order to work with subsequent print statements.\n",
    "- Extract the `rank` from the `best_model` by calling the `.getRank()` method on the `best_model`.\n",
    "- Extract the `maxIter` from the `best_model` by calling the `.getMaxIter()` method on the `best_model`.\n",
    "- Extract the `regParam` from the `best_model` by calling the `.getRegParam()` method on the `best_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ec8c6fc-c2b0-47e6-ad9e-38e7af41b73a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Fit cross validator to the 'train' dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#Extract best model from the cv model above\u001b[39;00m\n\u001b[0;32m      5\u001b[0m best_model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbestModel\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\ml\\tuning.py:838\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n\u001b[0;32m    836\u001b[0m     subModels \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(numModels)] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nFolds)]\n\u001b[1;32m--> 838\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kFold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nFolds):\n\u001b[0;32m    840\u001b[0m     validation \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\ml\\tuning.py:877\u001b[0m, in \u001b[0;36mCrossValidator._kFold\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    875\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m nFolds\n\u001b[0;32m    876\u001b[0m randCol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muid \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_rand\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 877\u001b[0m df \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39malias(randCol))\n\u001b[0;32m    878\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nFolds):\n\u001b[0;32m    879\u001b[0m     validateLB \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m*\u001b[39m h\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\sql\\utils.py:159\u001b[0m, in \u001b[0;36mtry_remote_functions.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\sql\\functions.py:3370\u001b[0m, in \u001b[0;36mrand\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m   3336\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generates a random column with independent and identically distributed (i.i.d.) samples\u001b[39;00m\n\u001b[0;32m   3337\u001b[0m \u001b[38;5;124;03muniformly distributed in [0.0, 1.0).\u001b[39;00m\n\u001b[0;32m   3338\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3367\u001b[0m \u001b[38;5;124;03m+---+------------------+\u001b[39;00m\n\u001b[0;32m   3368\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_invoke_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrand\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3371\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _invoke_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrand\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\sql\\functions.py:94\u001b[0m, in \u001b[0;36m_invoke_function\u001b[1;34m(name, *args)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03mInvokes JVM function identified by name with args\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03mand wraps the result with :class:`~pyspark.sql.Column`.\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m jf \u001b[38;5;241m=\u001b[39m \u001b[43m_get_jvm_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_active_spark_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jf(\u001b[38;5;241m*\u001b[39margs))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\sql\\functions.py:85\u001b[0m, in \u001b[0;36m_get_jvm_function\u001b[1;34m(name, sc)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03mRetrieves JVM function identified by name from\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03mJava gateway associated with sc.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctions\u001b[49m, name)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\py4j\\java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[0;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[1;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[0;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[0;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\py4j\\java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\py4j\\clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\py4j\\clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\py4j\\clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[1;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "#Fit cross validator to the 'train' dataset\n",
    "model = cv.fit(train)\n",
    "\n",
    "#Extract best model from the cv model above\n",
    "best_model = model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e72cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best_model\n",
    "print(type(best_model))\n",
    "\n",
    "# Complete the code below to extract the ALS model parameters\n",
    "print(\"**Best Model**\")\n",
    "\n",
    "# Print \"Rank\"\n",
    "print(\"  Rank:\", best_model.getRank())\n",
    "\n",
    "# Print \"MaxIter\"\n",
    "print(\"  MaxIter:\", best_model.getMaxIter())\n",
    "\n",
    "# Print \"RegParam\"\n",
    "print(\"  RegParam:\", best_model.getRegParam())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77216d0-3264-4d55-b0d3-361795ad6da4",
   "metadata": {},
   "source": [
    "# What if you don't have customer ratings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be6079f-c5c7-43e7-899e-7c85a7c96213",
   "metadata": {},
   "source": [
    "## Introduction to the Million Songs Dataset\n",
    "1. Introduction to the Million Songs Dataset\n",
    "\n",
    "By now you should be pretty comfortable with ALS. So far, we've only used explicit ratings. In most real-life situations, however, explicit ratings aren't available, and you'll have to get creative in building these types of models. One way to get around this is to use implicit ratings. Remember that while\n",
    "\n",
    "2. Explicit vs implicit\n",
    "\n",
    "explicit ratings are explicitly provided by users in various forms, implicit ratings are data used to infer ratings. For example, if a news website sees that in the last month you clicked on\n",
    "\n",
    "3. Explicit vs implicit (cont.)\n",
    "\n",
    "21 geopolitical articles and only 1 local news article, ALS can convert these numbers into scores indicating how confident it is that you like them. This approach assumes that the more you do something, the more you prefer it.\n",
    "\n",
    "4. Implicit refresher II\n",
    "\n",
    "ALS can use these confidence ratings to generate recommendations and you're going to learn how to do this. First, let's talk about the dataset you will be using.\n",
    "\n",
    "5. Introduction to the Million Songs Dataset\n",
    "\n",
    "The dataset this time comes from the Million Songs Dataset available from LabROSA at Columbia University. You're going to be using one file of this dataset called The Echo Nest Taste profile dataset. It contains information on over 1 million users including the number of times they've played nearly 400,000 songs. This is more data than we can use for this course, so we will only be using a portion of. We'll first examine the data, get summary statistics, and then build and evaluate our model. One thing to note here is that because the use of implicit ratings causes ALS to calculate a level of confidence that a user likes a song based on the number of times they've played it, the matrix will need to include zeros for the songs that each user has not yet listened to. In case your data doesn't already include the zeros, we'll walk through how to do this.\n",
    "\n",
    "6. Add zeros sample\n",
    "\n",
    "Let's say we have a ratings dataframe like this:\n",
    "\n",
    "7. Cross join intro\n",
    "\n",
    "You can use the .distinct() method to extract the unique userId's and songId's, like this.\n",
    "\n",
    "8. Cross join output\n",
    "\n",
    "You can then performs a cross join which joins each user to each song like this: Notice that the 3 users and 3 songs we originally had now create 9 unique pairs. Using a left join,\n",
    "\n",
    "9. Joining back original ratings data\n",
    "\n",
    "you can take that cross_join table, and join it with the original ratings to get the num_plays column. Notice it joins on both userId and songId. And because we want 0's in place of the null values, so that every user has a value for every song, we simply call the\n",
    "\n",
    "10. Filling in with zero\n",
    "\n",
    ".fillna() method telling Spark to fill the null values with 0. And you have your final product to feed to ALS.\n",
    "\n",
    "11. Add zeros function\n",
    "\n",
    "Here are all those steps in a clean function.\n",
    "\n",
    "12. Let's practice!\n",
    "\n",
    "Let's do this with our Million Songs dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbf5153",
   "metadata": {},
   "source": [
    "### Generate predictions and calculate RMSE\n",
    "Now that we have a model that is trained on our data and tuned through cross validation, we can see how it performs on the test dataframe. To do this, we'll calculate the RMSE.\n",
    "\n",
    "As a side note, the generation of test predictions takes more than a few minutes with this dataset. For this reason, the test predictions have been generated already and are provided here as a dataframe called `test_predictions`. For your reference, they are generated using this code: `test_predictions = best_model.transform(test)`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- The dataframe `test_predictions` contains predictions that our cross-validated ALS model generated using the test set that we created previously. Use the `.show()` method to take a look at it and see if the predictions seem close.\n",
    "- Use the evaluator that you built previously to calculate the RMSE by calling the `.evaluate()` method on the `test_predictions` generated. Call this `RMSE`.\n",
    "- Print the `RMSE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84efdc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = best_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f67cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the predictions \n",
    "test_predictions.show()\n",
    "\n",
    "# Calculate and print the RMSE of test_predictions\n",
    "RMSE = evaluator.evaluate(test_predictions)\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f87b617",
   "metadata": {},
   "source": [
    "### Interpreting the RMSE\n",
    "This model was able to achieve an `RMSE` of `0.633`. Click on the best interpretation of what this means.\n",
    "\n",
    "Answer the question\n",
    "\n",
    "- An RMSE of 0.633 means that the predictions are 6.33% off from the original values of the ratings matrix.\n",
    "\n",
    "- An RMSE of 0.633 means that 6.33% of the time, the recommendations generated by our ALS model will be wrong.\n",
    "\n",
    "- AN RMSE of 0.633 means that 6.33% of our users will receive recommendations that they won't take.\n",
    "\n",
    "- **An RMSE of 0.633 means that on average the model predicts 0.633 above or below values of the original ratings matrix.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7e5d59",
   "metadata": {},
   "source": [
    "### Do recommendations make sense\n",
    "Now that we have an understanding of how well our model performed, and have some confidence that it will provide recommendations that are relevant to users, let's actually look at recommendations made to a user and see if they make sense.\n",
    "\n",
    "The original ratings data is provided here as `original_ratings`. Take a look at user 60 and user 63's original ratings, and compare them to what ALS recommended for them. In your opinion, are the recommendations consistent with their original preferences?\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Use the `.filter()` on the `original_ratings` dataframe to ensure that `col(\"userId\")` equals `60` to look at user `60`'s original ratings. Call the `.sort()` method to sort the output by rating and set the `ascending` argument to `False` to see the highest ratings first.\n",
    "- Use the `.filter()` and `.show()` methods on the recommendations dataset to look at user `60`'s ratings. Note that when ALS generates recommendations, they are provided in descending order by `userId`, so there's no need to sort the dataframe.\n",
    "- Do the same thing for user `63`.\n",
    "- Do the recommendation genres have some consistency with each user's original ratings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75894768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at user 60's ratings\n",
    "print(\"User 60's Ratings:\")\n",
    "original_ratings.filter(col(\"userId\") == 60).sort(\"rating\", ascending = False).show()\n",
    "\n",
    "# Look at the movies recommended to user 60\n",
    "print(\"User 60s Recommendations:\")\n",
    "recommendations.filter(col(\"userId\") == 60).show()\n",
    "\n",
    "# Look at user 63's ratings\n",
    "print(\"User 63's Ratings:\")\n",
    "original_ratings.filter(col(\"userId\") == 63).sort(\"rating\", ascending = False).show()\n",
    "\n",
    "# Look at the movies recommended to user 63\n",
    "print(\"User 63's Recommendations:\")\n",
    "recommendations.filter(col(\"userId\") == 63).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f4b2c-ccbe-499c-b196-ecd49976888f",
   "metadata": {},
   "source": [
    "## Evaluating implicit ratings models\n",
    "1. Evaluating implicit ratings models\n",
    "\n",
    "Now that we have an implicit ratings dataset, let's discuss these types of models. The first thing you should know is that implicit ratings models have an additional hyperparameter called alpha. Alpha is an integer value that tells Spark how much each additional song play should add to the model's confidence that a user actually likes a song. Like the other hyperparameters, this will need to be tuned through cross validation. The challenge of these models is the evaluation. With explicit ratings, we used was the RMSE. It made sense in that situation because we could\n",
    "\n",
    "2. Why RMSE worked before\n",
    "\n",
    "match predictions back to a true measure of user preference. In the case of implicit ratings however,\n",
    "\n",
    "3. Why RMSE doesn't work now\n",
    "\n",
    "we don’t have a true measure of user preference. We only have the number of times a user listened to a song and a measure of how confident our model is that that they like that song. These aren't the same thing and calculating an RMSE between them doesn't make sense. However, using a test set, we can see if our model is giving high predictions to the songs that users have actually listened to. The logic being that if our model is returning a high prediction for a song that the respective user has actually listened to, then the predictions make sense, especially if they've listened to it more than once. We can measure this using this\n",
    "\n",
    "4. (ROEM) Rank Ordering Error Metric\n",
    "\n",
    "Rank Order Error Metric. In essence this metric checks to see if songs with higher numbers of plays have higher predictions.\n",
    "\n",
    "5. ROEM bad predictions\n",
    "\n",
    "For example, here is a set of bad predictions. The perc_rank column has ranked the predictions for each individual user such that the lowest prediction is in the highest precentile and the highest prediction is in the lowest percentile. Notice that these bad predictions include low predictions and high predictions for songs with more than one play indicating that the predictions may not be any better than random. If we multiply the number of plays by the percentRank, we get\n",
    "\n",
    "6. ROEM: PercRank * plays\n",
    "\n",
    "this np*rank column.\n",
    "\n",
    "7. ROEM: bad predictions\n",
    "\n",
    "When we sum that column we get our ROEM numerator{{1}}, and the sum of the numPlays column gives us our ROEM denominator. Using these, we can calculate our ROEM{{3}} to be 0.556. Values close to .5 indicate that they aren't much better than random. If we were to look at good predictions where the model gave high predictions to songs that had more than 1 play, they might look like this:\n",
    "\n",
    "8. Good predictions\n",
    "\n",
    "Notice that songs that have been played have high ratings indicating that the predictions are better than random. Which subsequently gives us an ROEM of\n",
    "\n",
    "9. ROEM: good predictions\n",
    "\n",
    "0.1111. This is much closer to 0, where we want to be. Unfortunately Spark hasn't implemented an evaluator metric like this, so you'll need to build it manually. An ROEM function will be provided to you in subsequent exercises. And for your reference, the code to build it is provided at the end of this course\n",
    "\n",
    "10. ROEM: link to function on GitHub\n",
    "\n",
    "Using this function, and a for loop\n",
    "\n",
    "11. Building several ROEM models\n",
    "\n",
    "you can build several models as you see here, each with different hyperparameter values. You'll want to create a model for each combination of hyperparameter values that you want to try.\n",
    "\n",
    "12. Error output\n",
    "\n",
    "You can then fit each one to the training data, extract each model's test predictions, and then calculate the ROEM for each one. This is a simplified way to do this. Full cross-validation is imperative to building good models. It is beyond the scope of this course to teach how to code a function that manually cross-validates and evaluates models like this, but doing so should be done, and code to do so is provided at the end of the course.\n",
    "\n",
    "13. Let's practice!\n",
    "\n",
    "Let's put this into practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635cbef8-13e6-4119-bbbc-ce284089c30e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74b0749d-9ec3-468e-9604-5a31374f8537",
   "metadata": {},
   "source": [
    "## Overview of binary, implicit ratings\n",
    "1. Overview of binary, implicit ratings\n",
    "\n",
    "So far we've covered sitautions when you have explicit ratings, and when you have implicit ratings from user behavior counts. Now we're going to cover the situation when you might not even have behavior counts. In some situations, you may only have binary data that tells you whether a user has or has not taken an action with no indication of how many times they've done so. To go back to the movie example, if you know whether customers have watched certain movies, but don't have information on how many times or how much they actually liked them, you could simply feed binary data to ALS that indicates which customers have watched each movie and which ones haven't. ALS can still pull signal from this type of data and make meaningful predictions. When taking this approach, the data will look like this.\n",
    "\n",
    "2. Binary ratings\n",
    "\n",
    "Notice that all ratings are either a 1 or a 0. We must treat binary ratings like these as implicit ratings. If we treated them like explicit ratings and didn't include the 0's, the best performing model would simply predict 1 for everything, and deliver a deceivingly ideal RMSE of 0. Also, as with our previous Million Songs model, we can't use the RMSE as a model evaluation metric. Ultimately, when our machine learning process holds out random observations in the test set, we want our model to generate high predictions for those movies that users have actually watched. For this reason, we'll use our ROEM metric again. We'll apply the same concepts we've covered previously on this binary dataset. The convenience of using the MovieLens dataset is that we can see how our binary model performs against the original, true preference ratings of the original MovieLens dataset.\n",
    "\n",
    "3. Class imbalance\n",
    "\n",
    "One word about binary models. While it's perfectly feasible to feed binary data like this into ALS and get meaningful recommendations, the data does have a sort of class imbalance where the vast majority of ratings are 0's with a small percentage of 1's. Since implicit ratings models use customized error metrics like ROEM and not RMSE, the class imbalance doesn't really pose a problem like it might in classification problems. ALS can still generate meaningful recommendations from this type of data but there are strategies that can be taken with the data to try and improve recommendations.\n",
    "\n",
    "4. Item weighting\n",
    "\n",
    "For example, rather than treat unseen movies purely as 0's, you can weight them higher if more people have seen them. This assumes that if many people have seen a movie, it must be a pretty good movie and therefore should be treated with a little more weight, and vice versa. This is called item weighting.\n",
    "\n",
    "5. Item weighting and user weighting\n",
    "\n",
    "Likewise you could weight movies by individual user behavior. For example, if a user has seen lots of movies, you could weight their unseen movies lower assuming that if a user has seen lots of movies, they know what they like and have deliberately chosen NOT to view the movies they haven't seen and therefore those movies deserve a lower weighting. While these methods are applicable, their methods haven't been implemented into the Pyspark framework, and therefore require a lot of manual work which is beyond the scope of this course. However, if you'd like to learn more about these types of approaches, you can read the paper referenced at the end of the course.\n",
    "\n",
    "6. Let's practice!\n",
    "\n",
    "Let's build a binary ratings model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3b7270-b31c-4b68-88c2-88cc1ef9950d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25cb0e65-9c94-402a-b1a2-e9f36a1db1f1",
   "metadata": {},
   "source": [
    "# Course recap\n",
    "1. Course recap\n",
    "\n",
    "Congratulations. You've now completed this course on building Collaborative Filtering recommendation engines in Pyspark. We've covered a number of things from why these are important to matrix multiplication and factorization and latent features. But most importantly, you've learned how to build and interpret recommendation engines with three different data types:\n",
    "\n",
    "2. Course summary\n",
    "\n",
    "* Explicit Ratings * Implicit Ratings using user behavior counts{{1}} * Implicit Ratings using binary user behavior{{2}} With this information you'll be well-prepared to build a collaborative-filtering recommendation engine with the relevant data available to you as a data scientist. Some things to bear in mind about these types of models:\n",
    "\n",
    "3. Things to bear in mind\n",
    "\n",
    "If users don't have a lot of ratings, and ALS can't infer much about them, it's likely that ALS will make broad general recommendations that aren't really personalized. You might have seen this if you spent extra time exploring some of the recommendation output. Like all models, the more data there is, the better the model performs.\n",
    "\n",
    "4. Things to bear in mind (cont.)\n",
    "\n",
    "While we've gone over different ways of evaluating recommendations engines, the only way to really know if your model performs well is to test it on users and see if they actually take your recommendations. It's entirely possible that a simple binary implicit ratings model provides better recommendations for users than an explicit model, but the only way to know is to test it. Bear this in mind as you move forward.\n",
    "\n",
    "5. Resources\n",
    "\n",
    "Here are some resources to help you as you continue to learn about these models and begin to build them on your own. The {{1}} first is a paper published by McKinsey and Company discussing the power of recommendation engines like ALS based models. The {[2}} second is the code to build the wide_to_long function discussed in the section about preparing data for ALS. The {[3}} third is the white paper that provides the academic background for building ALS models using implicit ratings. I highly recommend reading this paper as it provides a lot of context and insight into how these models work and alternative ways to evaluate them. The {{4}} fourth is a GitHub link for code that manages the cross validation and model evaluation for implicit ratings models using ALS in Pyspark. The {[5}} last resource listed here is a paper that discusses the math and intuition behind the user-based weighting and item-based weighting methodologies for addressing the class imbalance present in binary ratings models. Congratulations on completing this course, and best of luck as you move forward."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
